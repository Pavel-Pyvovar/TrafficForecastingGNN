{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554cdb78-6492-4009-8ced-3850315bc7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import libraries and define globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b68a678-4b38-4597-bffa-0d37001bbae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyrosm tqdm folium\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2a94cb-d732-4098-bc47-e3b9c444cf9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install -q torch-geometric-temporal==0.54.0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7334fe4c-cf68-4141-b7ec-bd11bc22ae1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import pyrosm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric_temporal.nn.recurrent import A3TGCN\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal, temporal_signal_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5601a8fb-1f67-4825-9fc3-25795253374c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9568591-cd8e-4ad7-8a19-96683bab7aec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CITY_ID = 1_000_000\n",
    "MAP_FILE = f\"{CITY_ID}-latest.osm.pbf\"\n",
    "LABEL = \"speed_kmh\"\n",
    "S3 = boto3.client('s3')\n",
    "S3_BUCKET = \"some_bucket\"\n",
    "S3_SUBDIR = f\"subdir_path\"\n",
    "S3_DATA = \"data_path\"\n",
    "S3_PREDS = f\"{S3_SUBDIR}/model_preds\"\n",
    "S3_FILENAME = \"edge_time_aggregated_4_lags.parquet\"\n",
    "N_WEEKS = 4\n",
    "N_WEEKS_TRAINING = 2\n",
    "N_WEEKS_VALIDATION = 1\n",
    "TRAIN_RATIO = N_WEEKS_TRAINING / N_WEEKS\n",
    "EPOCHS = 100\n",
    "HEADS = 8\n",
    "DROPOUT=0\n",
    "LEARNING_RATE = 1e-3\n",
    "HIDDEN_CHANNELS = 8\n",
    "OUT_CHANNELS = 1\n",
    "LOG_FREQ = 1\n",
    "EARLY_STOP_THRESHOLD = 5\n",
    "DATA_SPLITS = [\"train\", \"valid\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742d19bf-8a64-4a67-8d3d-f16a1250cfe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/unique_edges.pickle\", \"unique_edges.pickle\")\n",
    "with open(\"unique_edges.pickle\", \"rb\") as f:\n",
    "    UNIQUE_EDGES = pickle.load(f)\n",
    "len(UNIQUE_EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e61fb9-0727-4171-a030-dfa63743f2ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = f\"gnn_2_gats_{EPOCHS}_hidden_channels_{HIDDEN_CHANNELS}_epochs_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks\"\n",
    "GNN_DATASET_NAME = f\"gnn_dataset_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks_normalised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99282d87-9c82-4b9a-9812-af9cf781c47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix():\n",
    "    adjacency_matrix = np.zeros((len(UNIQUE_EDGES), len(UNIQUE_EDGES)))\n",
    "\n",
    "    for i, edge_i in enumerate(UNIQUE_EDGES):\n",
    "        for j, edge_j in enumerate(UNIQUE_EDGES):\n",
    "            if set(edge_i).intersection(set(edge_j)):\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1\n",
    "\n",
    "    adjacency_matrix = adjacency_matrix.astype(np.float32)\n",
    "    edge_index = (np.array(adjacency_matrix) > 0).nonzero()\n",
    "    return adjacency_matrix, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08e54dd-f619-4bff-a416-2c3292c63056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35aa2bec-ab60-412d-ba77-a93db266a933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fallback_to_past(edge, minute_bucket, fallback_horizon, unit='m'):\n",
    "    return DATASET_DICT.get((edge, minute_bucket - pd.Timedelta(fallback_horizon, unit=unit)))\n",
    "\n",
    "\n",
    "def neighbour_average(edge, minute_bucket):\n",
    "    neighbour_indicies = np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]\n",
    "    neighbour_speeds = []\n",
    "    for idx in neighbour_indicies:\n",
    "        speed = DATASET_DICT.get((edge, minute_bucket))\n",
    "        if speed is None or math.isnan(speed):\n",
    "            continue\n",
    "        neighbour_speeds.append(speed)\n",
    "    return np.mean(neighbour_speeds)\n",
    "\n",
    "\n",
    "def expand_edge_time_series(edge_df):\n",
    "    edge_df = (edge_df.reset_index().set_index(\"minute_bucket\")\n",
    "        .join(DATASET_RANGE_DF, how=\"right\", lsuffix='l')\n",
    "        .drop([\"index\", \"indexl\"], axis=1))\n",
    "    edge_df[\"edge\"] = edge_df.edge.ffill().bfill()\n",
    "    edge_df = edge_df.reset_index()\n",
    "    return edge_df\n",
    "    \n",
    "\n",
    "def compute_rolling_mean(speeds_df, window):\n",
    "    rolling_window_speed_avg_df = (\n",
    "        pd.concat([expand_edge_time_series(g) for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].groupby(\"edge\")])\n",
    "        .set_index(\"minute_bucket\").groupby(\"edge\").rolling(window).mean())\n",
    "    rolling_window_speed_avg_df.dropna(inplace=True)\n",
    "    return rolling_window_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "\n",
    "\n",
    "def impute_nan(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    for horizon, unit in [(1, 'W'), (2, 'W')]:\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = fallback_to_past(edge, minute_bucket, 15, 'm')\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "\n",
    "    if speed is None:\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_15_MIN_BUCKET_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "    else:\n",
    "        return speed\n",
    "    return speed\n",
    "\n",
    "\n",
    "def impute_dataset(speeds_df, imputation_method):\n",
    "    \"\"\"Iterate over a speeds data frame in 15-minute interval groups, fill missing values, collect into a list of snapshots.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    feature_imputation_count = 0\n",
    "    target_imputation_count = 0\n",
    "    target_mask = np.ones((len(DATASET_DATE_RANGE), len(UNIQUE_EDGES)), dtype=int)\n",
    "    for i, (minute_bucket, minute_bucket_group) in enumerate(tqdm(speeds_df.groupby(\"minute_bucket\"))):\n",
    "        edge_dict = minute_bucket_group[[\"edge\", \"speed_kmh\"] + SPEED_FEATURES].set_index(\"edge\").to_dict()\n",
    "        measurements = []\n",
    "        targets = []\n",
    "        past_hour = [(minute, minute_bucket - pd.to_timedelta(minute, unit='m')) for minute in [15, 30, 45, 60]]\n",
    "        next_15 = minute_bucket + pd.to_timedelta(15, unit='m')\n",
    "        for j, edge in enumerate(UNIQUE_EDGES):\n",
    "            row = []\n",
    "            for minute, quarter in past_hour:\n",
    "                speed = edge_dict[f\"speed_kmh_lag_{minute}_m\"].get(edge)\n",
    "                if speed is None or math.isnan(speed):\n",
    "                    speed = imputation_method(edge, quarter)\n",
    "                    feature_imputation_count += 1\n",
    "                row.append(speed)\n",
    "            measurements.append(row)\n",
    "            speed = edge_dict[\"speed_kmh\"].get(edge)\n",
    "            if speed is None or math.isnan(speed):\n",
    "                # TODO: not the most efficient way of skipping unpopular segments\n",
    "                # These are the segments that linear regression couldn't be trained on due to insufficient amount of data\n",
    "                speed = imputation_method(edge, next_15)\n",
    "                target_imputation_count += 1\n",
    "                target_mask[i, j] = 0\n",
    "            targets.append(speed)\n",
    "        xs.append(measurements)\n",
    "        ys.append(targets)\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ys = np.array(ys, dtype=np.float32)\n",
    "\n",
    "    print(f\"Feature imputation count: {feature_imputation_count}\")\n",
    "    print(f\"Target imputation count: {target_imputation_count}\")\n",
    "    print(f\"Total number of values: {len(UNIQUE_EDGES) * len(DATASET_DATE_RANGE) * 5}\")\n",
    "    print()\n",
    "\n",
    "    return xs, ys, target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e3da01-c0c2-47f0-a266-369b1a9421fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314ddec2-891c-4b70-83b7-7fa5f7728c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_global_mean_baseline(dataset):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for snapshot in dataset:\n",
    "        mse += (((MEAN_SPEED - snapshot.y)*snapshot.mask)**2).sum() / snapshot.mask.sum()\n",
    "        mae += (np.abs((MEAN_SPEED - snapshot.y)*snapshot.mask)).sum() / snapshot.mask.sum()\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def evaluate_edge_average_baseline(dataset):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for snapshot in dataset:\n",
    "        snapshot_mse = 0\n",
    "        snapshot_mae = 0\n",
    "        for i, edge in enumerate(UNIQUE_EDGES):\n",
    "            snapshot_mse += snapshot.mask[i] * (EDGE_AVG_DICT.get(edge, MEAN_SPEED) - snapshot.y[i])**2\n",
    "            snapshot_mae += snapshot.mask[i] * np.abs(EDGE_AVG_DICT.get(edge, MEAN_SPEED) - snapshot.y[i])\n",
    "        snapshot_mse /= snapshot.mask.sum()\n",
    "        snapshot_mae /= snapshot.mask.sum()\n",
    "        mse += snapshot_mse\n",
    "        mae += snapshot_mae\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def edge_time_naive(edge, timestamp):\n",
    "    weekday = timestamp.weekday()\n",
    "    hour = timestamp.hour\n",
    "    minute = timestamp.minute\n",
    "    return EDGE_15_MIN_BUCKET_DICT.get((edge, weekday, hour, minute), EDGE_AVG_DICT.get(edge, MEAN_SPEED))\n",
    "\n",
    "\n",
    "def rolling_edge_time_avg_naive(edge, minute_bucket):\n",
    "    return ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket), MINUTE_BUCKET_AVG_DICT.get((minute_bucket - pd.Timedelta(15, unit='m')), MEAN_SPEED))\n",
    "\n",
    "\n",
    "def evaluate_edge_time_average_baseline(dataset, date_range, naive):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for timestamp, snapshot in zip(date_range, dataset):\n",
    "        snapshot_mse = 0\n",
    "        snapshot_mae = 0\n",
    "        for i, edge in enumerate(UNIQUE_EDGES):\n",
    "            snapshot_mse += snapshot.mask[i] * (naive(edge, timestamp) - snapshot.y[i])**2\n",
    "            snapshot_mae += snapshot.mask[i] * np.abs(naive(edge, timestamp) - snapshot.y[i])\n",
    "        snapshot_mse /= snapshot.mask.sum()\n",
    "        snapshot_mae /= snapshot.mask.sum()\n",
    "        mse += snapshot_mse\n",
    "        mae += snapshot_mae\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0894fd41-baa8-4a4f-82d2-438fce864be5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# GNN training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa86fe1f-104f-402f-b966-d166c933fe03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TemporalGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, hidden_channels, periods):\n",
    "        super().__init__()\n",
    "        self.tgnn = A3TGCN(in_channels=dim_in, out_channels=hidden_channels, periods=periods)\n",
    "        self.linear = torch.nn.Linear(hidden_channels, periods)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.tgnn(x, edge_index).relu()\n",
    "        h = self.linear(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "def plot_curves(losses):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    ax[0].plot(range(len(losses)), losses, label=[\"Train\", \"Validation\"])\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Mean Squared Error\")\n",
    "    ax[0].set_title(\"Learning curves for a simple GNN\")\n",
    "\n",
    "    # Plot the second half of the learning curve to avoid the huge spikes at the beginning of training\n",
    "    ax[1].plot(range(len(losses)//2, len(losses), 1), losses[len(losses)//2:], label=[\"Train\", \"Validation\"])\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Mean Squared Error\")\n",
    "    ax[1].set_title(f\"Learning curves for a simple GNN starting from epoch {len(losses)//2}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(train_dataset, valid_dataset, epochs=10):\n",
    "    model = TemporalGNN(len(SPEED_FEATURES), HIDDEN_CHANNELS, OUT_CHANNELS)\n",
    "\n",
    "    optimiser = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)\n",
    "    # scheduler = lr_scheduler.LinearLR(optimiser, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
    "    # scheduler = lr_scheduler.ExponentialLR(optimiser, gamma=0.99)\n",
    "    # mse = torch.nn.MSELoss(reduction=\"sum\")\n",
    "    # mae = torch.nn.L1Loss(reduction=\"sum\")\n",
    "    mse = torch.nn.MSELoss()\n",
    "    mae = torch.nn.L1Loss()\n",
    "    model.train()\n",
    "\n",
    "    best_val_mse = 1_000_000\n",
    "    best_epoch = -1\n",
    "    mse_losses = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        mse_loss = 0\n",
    "        mae_loss = 0\n",
    "        # sample_cnt = 0\n",
    "        for snapshot in train_dataset:\n",
    "            mask = snapshot.mask == 1\n",
    "            # sample_cnt += mask.sum()\n",
    "            y_pred = model(snapshot.x.unsqueeze(2), snapshot.edge_index)\n",
    "            # mse_loss += torch.sum((y_pred.flatten()[mask] - snapshot.y[mask])**2)\n",
    "            # mae_loss += torch.sum(torch.abs(y_pred.flatten()[mask] - snapshot.y[mask]))\n",
    "            mse_loss += mse(y_pred.flatten()[mask], snapshot.y[mask])\n",
    "            mae_loss += mae(y_pred.flatten()[mask], snapshot.y[mask])\n",
    "            # mse_loss += mse(y_pred.flatten(), snapshot.y)\n",
    "            # mae_loss += mae(y_pred.flatten(), snapshot.y)\n",
    "        mse_loss /= train_dataset.snapshot_count\n",
    "        mae_loss /= train_dataset.snapshot_count\n",
    "        # mse_loss /= sample_cnt\n",
    "        # mae_loss /= sample_cnt\n",
    "        mse_loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        val_mse_loss = 0\n",
    "        val_mae_loss = 0\n",
    "        # sample_cnt = 0\n",
    "        for snapshot in valid_dataset:\n",
    "            mask = snapshot.mask == 1\n",
    "            # sample_cnt += mask.sum()\n",
    "            y_pred = model(snapshot.x.unsqueeze(2), snapshot.edge_index)\n",
    "            val_mse_loss += mse(y_pred.flatten()[mask], snapshot.y[mask])\n",
    "            val_mae_loss += mae(y_pred.flatten()[mask], snapshot.y[mask])\n",
    "            # val_mse_loss += mse(y_pred.flatten(), snapshot.y)\n",
    "            # val_mae_loss += mae(y_pred.flatten(), snapshot.y)\n",
    "            # val_mse_loss += torch.sum((y_pred.flatten()[mask] - snapshot.y[mask])**2)\n",
    "            # val_mae_loss += torch.sum(torch.abs(y_pred.flatten()[mask] - snapshot.y[mask]))\n",
    "        val_mse_loss /= valid_dataset.snapshot_count\n",
    "        val_mae_loss /= valid_dataset.snapshot_count\n",
    "        # val_mse_loss /= sample_cnt\n",
    "        # val_mae_loss /= sample_cnt\n",
    "\n",
    "        # scheduler.step()\n",
    "\n",
    "        if epoch % LOG_FREQ == 0:\n",
    "            print(f\"Epoch {epoch:>2} | Train MSE: {mse_loss:.4f} | Train MAE: {mae_loss:.4f} | Valid MSE: {val_mse_loss:.4f} | Valid MAE: {val_mae_loss:.4f}\")\n",
    "\n",
    "        mse_losses.append((mse_loss.detach().numpy(), val_mse_loss.detach().numpy()))\n",
    "\n",
    "        if val_mse_loss < best_val_mse:\n",
    "            best_epoch = epoch\n",
    "            save_model(model, MODEL_NAME)\n",
    "        elif epoch - best_epoch > EARLY_STOP_THRESHOLD:\n",
    "            print(f\"Early stopped training at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    plot_curves(mse_losses)\n",
    "    return model\n",
    "\n",
    "    \n",
    "def save_model(model, model_name):\n",
    "    torch.save(model.state_dict(), f\"{model_name}.pt\")\n",
    "    S3.upload_file(f\"{model_name}.pt\", S3_BUCKET, f\"{S3_SUBDIR}/models/gnn/{model_name}.pt\")\n",
    "\n",
    "\n",
    "def inference(model, test_dataset, aggregate_by_snapshot=True):\n",
    "    if aggregate_by_snapshot:\n",
    "        mse = torch.nn.MSELoss()\n",
    "        mae = torch.nn.L1Loss()\n",
    "    else:\n",
    "        mse = torch.nn.MSELoss(reduction=\"sum\")\n",
    "        mae = torch.nn.L1Loss(reduction=\"sum\")\n",
    "\n",
    "    mse_loss = 0\n",
    "    mae_loss = 0\n",
    "    sample_cnt = 0\n",
    "    model.eval()\n",
    "    for snapshot in test_dataset:\n",
    "        mask = snapshot.mask == 1\n",
    "        sample_cnt += mask.sum()\n",
    "        y_pred = denormalise(model(snapshot.x.unsqueeze(2), snapshot.edge_index).flatten()[mask])\n",
    "        y_true = denormalise(snapshot.y[mask])\n",
    "        mse_loss += mse(y_pred, y_true)\n",
    "        mae_loss += mae(y_pred, y_true)\n",
    "\n",
    "    if aggregate_by_snapshot:\n",
    "        mse_loss /= train_dataset.snapshot_count\n",
    "        mae_loss /= train_dataset.snapshot_count\n",
    "    else:\n",
    "        mse_loss /= sample_cnt\n",
    "        mae_loss /= sample_cnt\n",
    "\n",
    "    return mse_loss.item(), mae_loss.item()\n",
    "\n",
    "\n",
    "def model_predict(model, dataset, edge):\n",
    "    edge_predictions = []\n",
    "    model.eval()\n",
    "    for snapshot in dataset:\n",
    "        y_pred = model(snapshot.x.unsqueeze(2), snapshot.edge_index)\n",
    "        edge_predictions.append(y_pred.detach().numpy()[EDGE_IDX_MAP[edge]])\n",
    "    return np.array(edge_predictions).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e781100-6ddd-480c-8b45-cc5767a4bcaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fa0054-e795-4882-9063-1b632c6e3cb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_random_edge_and_neighbours_time_series(speeds_df, dataset, model, nodes):\n",
    "    edge = random.choice(UNIQUE_EDGES)\n",
    "    neighbours = [IDX_EDGE_MAP[idx] for idx in np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]]\n",
    "    neighbours.remove(edge)\n",
    "    for e in [edge] + neighbours:\n",
    "        plot_edge_time_series(e, speeds_df, dataset, model)\n",
    "    return plot_edges(nodes, [edge] + neighbours)\n",
    "    \n",
    "\n",
    "def plot_edge_and_neighbours_time_series(edge, speeds_df, dataset, model, nodes):\n",
    "    neighbours = [IDX_EDGE_MAP[idx] for idx in np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]]\n",
    "    neighbours.remove(edge)\n",
    "    for e in [edge] + neighbours:\n",
    "        plot_edge_time_series(e, speeds_df, dataset, model)\n",
    "    return plot_edges(nodes, [edge] + neighbours)\n",
    "    \n",
    "\n",
    "def plot_edge_time_series(edge, speeds_df, dataset, model):\n",
    "    one_edge_df = speeds_df[speeds_df.edge == edge][[\"minute_bucket\", \"speed_kmh\"]].sort_values(\"minute_bucket\")\n",
    "    ys = dataset.targets\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "        x=one_edge_df.minute_bucket,\n",
    "        y=one_edge_df.speed_kmh,\n",
    "        mode='markers',\n",
    "        name='Ground Truth'\n",
    "    ))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "        x=DATASET_DATE_RANGE,\n",
    "        y=[y[EDGE_IDX_MAP[edge]] for y in ys],\n",
    "        mode='markers',\n",
    "        name='Imputed'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=DATASET_DATE_RANGE,\n",
    "        y=model_predict(model, dataset, edge),\n",
    "        mode='markers',\n",
    "        name='GNN predictions'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=DATASET_DATE_RANGE,\n",
    "        y=[edge_time_naive(edge, ts) for ts in DATASET_DATE_RANGE],\n",
    "        mode='markers',\n",
    "        name='Naive predictions'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"Time series for edge {edge}\",\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(\n",
    "            title=\"Time [15-minute bucket]\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Speed [km/h]\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update layout with legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_edges(nodes, edges):\n",
    "    m = folium.Map(location=[44.435608, 26.102297], zoom_start=15)\n",
    "\n",
    "    node_ids = [n for edge in edges for n in edge]\n",
    "\n",
    "    # Add edges to the map\n",
    "    for u, v in edges:\n",
    "        x0, y0 = nodes[nodes[\"id\"] == u][[\"lat\", \"lon\"]].iloc[0]\n",
    "        x1, y1 = nodes[nodes[\"id\"] == v][[\"lat\", \"lon\"]].iloc[0]\n",
    "        folium.PolyLine(locations=[(x0, y0), (x1, y1)], color='blue', weight=5, tooltip=f\"{u, v}\").add_to(m)\n",
    "\n",
    "    # Add nodes to the map\n",
    "    for node in node_ids:\n",
    "        x, y = nodes[nodes[\"id\"] == node][[\"lat\", \"lon\"]].iloc[0]\n",
    "        folium.CircleMarker(location=(x, y), radius=5, color='red', fill=True, fill_color='red').add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed00397-ffca-428c-87a2-c936ac3e6367",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5d915d-33d2-426b-a4bb-d147a992d16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    return (x - MEAN) / STD\n",
    "\n",
    "\n",
    "def denormalise(x):\n",
    "    return x * STD + MEAN\n",
    "    \n",
    "\n",
    "def extract_city_graph():\n",
    "    S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{CITY_ID}-latest.osm.pbf\", \"bucharest.pbf\")\n",
    "\n",
    "    osm = pyrosm.OSM(\"bucharest.pbf\")\n",
    "    nodes, edges = osm.get_network(nodes=True, network_type=\"driving+service\")\n",
    "    edges[\"edge\"] = list(zip(edges.u, edges.v))\n",
    "    print(f\"Unique OSM nodes: {nodes.id.nunique()}, unique OSM edges: {edges.id.nunique()}\")\n",
    "\n",
    "    if not os.path.isfile(S3_FILENAME):\n",
    "        S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/{S3_FILENAME}\", S3_FILENAME)\n",
    "    \n",
    "    speeds_df = pd.read_parquet(S3_FILENAME)\n",
    "\n",
    "    print(f\"Dataset time boundaries: {speeds_df.minute_bucket.min(), speeds_df.minute_bucket.max()}\")\n",
    "    print(f\"Initial dataset shape: {speeds_df.shape}\")\n",
    "\n",
    "    speeds_df[\"edge\"] = list(zip(speeds_df.start_node, speeds_df.end_node))\n",
    "\n",
    "    speeds_df = speeds_df[speeds_df.edge.isin(UNIQUE_EDGES)]\n",
    "\n",
    "    print(f\"Dataset shape after filtering edges of interest: {speeds_df.shape}\")\n",
    "\n",
    "    speeds_df[\"day\"] = speeds_df.minute_bucket.dt.weekday\n",
    "    speeds_df[\"hour\"] = speeds_df.minute_bucket.dt.hour\n",
    "    speeds_df[\"minute\"] = speeds_df.minute_bucket.dt.minute\n",
    "    speeds_df.sort_values([\"edge\", \"minute_bucket\"], inplace=True)\n",
    "\n",
    "    return speeds_df, nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5e1aa4-424a-4b2c-8123-2d0bafd66490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experimentation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9569a3-21cd-44dc-bbfd-68ef2d365e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(speeds_df, imputation_method):\n",
    "    if os.path.isfile(f\"{GNN_DATASET_NAME}.pickle\"):\n",
    "        with open(f\"{GNN_DATASET_NAME}.pickle\", \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        print(\"Loadeded imputed data\")\n",
    "    else:\n",
    "        print(\"Running data imputation ...\")\n",
    "        xs, ys, target_mask = impute_dataset(speeds_df, imputation_method)\n",
    "        dataset = StaticGraphTemporalSignal(EDGE_INDEX, ADJACENCY_MATRIX[ADJACENCY_MATRIX>0], xs, ys, mask=target_mask)\n",
    "        with open(f\"{GNN_DATASET_NAME}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        S3.upload_file(f\"{GNN_DATASET_NAME}.pickle\", S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/gnn/{GNN_DATASET_NAME}.pickle\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def evaluate_baselines(train_dataset, valid_dataset, test_dataset):\n",
    "    for naive_name, naive_method in zip([\"Global mean\", \"Edge mean\"], [evaluate_global_mean_baseline, evaluate_edge_average_baseline]):\n",
    "        for split, ds in zip([\"train\", \"valid\", \"test\"], [train_dataset, valid_dataset, test_dataset]):\n",
    "            mse, mae = naive_method(ds)\n",
    "            print(f\"\\t {naive_name} {split} MSE {mse:.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} RMSE {np.sqrt(mse):.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} MAE {mae:.{2}f}\")\n",
    "\n",
    "    for naive_name, naive_method in zip([\"Edge time naive\", \"Edge time rolling\"], [edge_time_naive, rolling_edge_time_avg_naive]):\n",
    "        for split, date_range, ds in zip([\"train\", \"valid\", \"test\"], [TRAIN_DATE_RANGE, VALID_DATE_RANGE, VALID_DATE_RANGE], [train_dataset, valid_dataset, test_dataset]):\n",
    "            mse, mae = evaluate_edge_time_average_baseline(ds, date_range, naive_method)\n",
    "            print(f\"\\t {naive_name} {split} MSE {mse:.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} RMSE {np.sqrt(mse):.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} MAE {mae:.{2}f}\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    train_dataset, valid_dataset = temporal_signal_split(dataset, train_ratio=TRAIN_RATIO)\n",
    "    valid_dataset, test_dataset = temporal_signal_split(valid_dataset, train_ratio=1/2) # Assume valid and test dataset are of equal length\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def calc_model_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "\n",
    "def run_experiment(dataset, run_baselines=True):\n",
    "    print(\"Example StaticGraphTemporalSignal snapshot:\")\n",
    "    print(dataset[0], '\\n')\n",
    "    \n",
    "    train_dataset, valid_dataset, test_dataset = split_dataset(dataset)\n",
    "    print(\"Train, valid and test set snapshot counts respectively: \", train_dataset.snapshot_count, valid_dataset.snapshot_count, test_dataset.snapshot_count, '\\n')\n",
    "\n",
    "    if run_baselines:\n",
    "        print(\"Evaluating baselines:\")\n",
    "        evaluate_baselines(train_dataset, valid_dataset, test_dataset)\n",
    "        print()\n",
    "\n",
    "    print(\"Training a GNN ...\")\n",
    "    model = train(train_dataset, valid_dataset, epochs=EPOCHS)\n",
    "    print()\n",
    "\n",
    "    params = calc_model_params(model)\n",
    "    print(\"Number of parameters in the model: \", params, '\\n')\n",
    "\n",
    "    mse, mae = inference(model, test_dataset)\n",
    "    print(\"Test MSE: \", mse)\n",
    "    print(\"Test RMSE: \", np.sqrt(mse))\n",
    "    print(\"Test MAE: \", mae)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_predictions(dataset, model, date_range):\n",
    "    edge_predictions = []\n",
    "    for minute_bucket, snapshot in zip(date_range, dataset):\n",
    "        mask = snapshot.mask == 1\n",
    "        used_edges = np.array(list(UNIQUE_EDGES))[mask]\n",
    "        edge_predictions.append(pd.DataFrame({\n",
    "            \"start_node\": used_edges[:, 0],\n",
    "            \"end_node\": used_edges[:, 1],\n",
    "            \"minute_bucket\": np.repeat(minute_bucket, mask.sum()),\n",
    "            f\"{MODEL_NAME}_speed\": denormalise(model(snapshot.x, snapshot.edge_index).flatten()[mask].detach().numpy()),\n",
    "            \"speed_kmh\": denormalise(snapshot.y[mask].detach().numpy())\n",
    "        }))\n",
    "    return pd.concat(edge_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b524898-ad63-4d2b-a144-64c8f27eba80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb37e2f-3a0d-476c-8504-b04bd6a0dd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df, nodes, edges = extract_city_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4025663b-2cfd-4f4c-a635-a2991d3e29df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATASET_START_DATE = subgraph_speeds_df.minute_bucket.min()\n",
    "DATASET_END_DATE = subgraph_speeds_df.minute_bucket.max()\n",
    "TRAIN_DATE_RANGE = pd.date_range(DATASET_START_DATE, DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W'), freq=\"15min\", inclusive=\"left\")\n",
    "VALID_DATE_RANGE = pd.date_range(TRAIN_DATE_RANGE[-1], TRAIN_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")\n",
    "TEST_DATE_RANGE = pd.date_range(VALID_DATE_RANGE[-1], VALID_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")\n",
    "DATASET_DATE_RANGE = pd.concat([TRAIN_DATE_RANGE.to_series(), VALID_DATE_RANGE.to_series(), TEST_DATE_RANGE.to_series()])\n",
    "DATASET_RANGE_DF = pd.DataFrame(DATASET_DATE_RANGE, columns=[\"minute_bucket\"]).reset_index().set_index(\"minute_bucket\")\n",
    "\n",
    "SPEED_FEATURES = [col_name for col_name in subgraph_speeds_df.columns if \"lag\" in col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827b34f2-7d43-4047-8d58-019e9ac5b22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket <= TEST_DATE_RANGE[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380d998a-5cb1-4f32-bb4f-0ba9bf7cbcb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0fb1d4-ee7d-4359-a16f-f0a7ce2241c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket < DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W')]\n",
    "MEAN = train_subgraph_speeds_df.speed_kmh.mean()\n",
    "STD = train_subgraph_speeds_df.speed_kmh.std()\n",
    "MEAN, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6610c6d-17ad-458d-8e92-39c0f23b4ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df[\"speed_kmh\"] = subgraph_speeds_df.speed_kmh.apply(lambda x: normalise(x))\n",
    "for feat in SPEED_FEATURES:\n",
    "    subgraph_speeds_df[feat] = subgraph_speeds_df[feat].apply(lambda x: normalise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa76e4a9-1d92-4a87-857b-47557409dea8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket < DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33f822f-f725-4766-84e5-887bc00821ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDGE_IDX_MAP = {edge: i for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "IDX_EDGE_MAP = {i: edge for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "\n",
    "MEAN_SPEED = train_subgraph_speeds_df.speed_kmh.mean()\n",
    "EDGE_AVG_DICT = train_subgraph_speeds_df[[\"speed_kmh\", \"edge\"]].groupby(\"edge\").mean().astype(int).to_dict()[\"speed_kmh\"]\n",
    "EDGE_15_MIN_BUCKET_DICT = train_subgraph_speeds_df.groupby([\"edge\", \"day\", \"hour\", \"minute\"])[\"speed_kmh\"].mean().to_dict()\n",
    "\n",
    "with open(\"edge_15min_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(EDGE_15_MIN_BUCKET_DICT, f)\n",
    "\n",
    "S3.upload_file(\"edge_15min_dict.pickle\", S3_BUCKET, f\"{S3_SUBDIR}/models/edge_15min_dict.pickle\")\n",
    "\n",
    "ADJACENCY_MATRIX, EDGE_INDEX = compute_adjacency_matrix()\n",
    "\n",
    "rolling_speed_avg_df = (pd.concat([expand_edge_time_series(g)\n",
    "    for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]]\n",
    "    .groupby(\"edge\")]).set_index(\"minute_bucket\").groupby(\"edge\").expanding().mean())\n",
    "rolling_speed_avg_df.dropna(inplace=True)\n",
    "ROLLING_EDGE_TIME_AVG_DICT = rolling_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "# TODO: Move these to data imputation methods\n",
    "ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"1h\")\n",
    "ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"2h\")\n",
    "ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"3h\")\n",
    "ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"4h\")\n",
    "\n",
    "DATASET_DICT = subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].set_index([\"edge\", \"minute_bucket\"]).to_dict()[\"speed_kmh\"]\n",
    "MINUTE_BUCKET_AVG_DICT = subgraph_speeds_df[[\"minute_bucket\", \"speed_kmh\"]].groupby(\"minute_bucket\").mean().to_dict()[\"speed_kmh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "141ed09a-4355-49ab-ba5e-23859c7f699b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GNN_DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488995f9-0931-4cc9-8bf4-03cd256c5b97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/gnn/{GNN_DATASET_NAME}.pickle\", f\"{GNN_DATASET_NAME}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf931e0-6ff4-473b-8a86-cee1b0a41640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rm \"{GNN_DATASET_NAME}.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1935bbe6-a23a-4aa8-b01b-26a76d72cf77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df.speed_kmh.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e67407c-23c0-4bcc-b310-5c3c842f524e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(subgraph_speeds_df, impute_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33be4457-9c42-4db3-8c89-b16827bf62b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768b2e42-49da-4bf4-9ff3-0a01e2723bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fa2ce8b-f515-4568-8b4a-5c21ad428ccb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loss average over snapshots, 100 epochs, 32 hidden channels, 2 week of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5463fee-de34-48c2-beae-1d8ad5265535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_CHANNELS = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0.2\n",
    "MODEL_NAME = f\"at3gcn_{EPOCHS}_epochs_{HIDDEN_CHANNELS}_hidden_channels_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3c9fb7-73a7-44bc-8a39-4452cbd2c3a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = run_experiment(dataset, run_baselines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0071eba-1fdc-409a-aa1f-2e1f2ea76682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These metrics contain masking\n",
    "print(\"Train: \", inference(model, train_dataset))\n",
    "print(\"Valid: \", inference(model, valid_dataset))\n",
    "print(\"Test: \", inference(model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8541e386-3c2a-4f25-b6bd-34b2374d54ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These metrics contain masking\n",
    "print(\"Train: \", inference(model, train_dataset, aggregate_by_snapshot=False))\n",
    "print(\"Valid: \", inference(model, valid_dataset, aggregate_by_snapshot=False))\n",
    "print(\"Test: \", inference(model, test_dataset, aggregate_by_snapshot=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b23979f-dd02-4979-85fa-aa4cf7d2c614",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loss average over snapshots, 100 epochs, 32 hidden channels, 2 weeks of training data, with target masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6798a34-2597-4c59-a484-9b4d13d4ca07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_CHANNELS = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0.2\n",
    "MODEL_NAME = f\"at3gcn_{EPOCHS}_epochs_{HIDDEN_CHANNELS}_hidden_channels_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks_with_target_masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c7a4f8-de62-4dee-825f-2d473b85d39c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = run_experiment(dataset, run_baselines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6837ca5f-e14a-459c-a68e-2a5e9eb5449f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These metrics contain masking\n",
    "print(\"Train: \", inference(model, train_dataset))\n",
    "print(\"Valid: \", inference(model, valid_dataset))\n",
    "print(\"Test: \", inference(model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c120b1d3-243a-4574-bdfd-82a0e8712b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These metrics contain masking\n",
    "print(\"Train: \", inference(model, train_dataset, aggregate_by_snapshot=False))\n",
    "print(\"Valid: \", inference(model, valid_dataset, aggregate_by_snapshot=False))\n",
    "print(\"Test: \", inference(model, test_dataset, aggregate_by_snapshot=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b64103-4c20-4f30-812a-9f2c8dac576b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9acea595-9e58-46fe-b345-5ce8bcc1330d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/gnn/{GNN_DATASET_NAME}.pickle\", f\"{GNN_DATASET_NAME}.pickle\")\n",
    "# S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/models/gnn/{MODEL_NAME}.pt\", f\"{MODEL_NAME}.pt\")\n",
    "\n",
    "# with open(f\"{GNN_DATASET_NAME}.pickle\", \"rb\") as file:\n",
    "#     dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8292ab36-f3c2-447c-8153-eed0ef5e3b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54b4a85-19cc-40e5-aea2-9553548f0cac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = GAT(len(SPEED_FEATURES), 8, 1)\n",
    "model.load_state_dict(torch.load(f\"{MODEL_NAME}.pt\"))\n",
    "train_dataset, valid_dataset, test_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ba0fb1-2938-4d6b-bc45-f7b3d883c018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(inference(model, train_dataset))\n",
    "# print(inference(model, valid_dataset))\n",
    "# print(inference(model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20dbbee-bf4d-4868-84be-efc8c89f5f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, valid_dataset, test_dataset = split_dataset(dataset)\n",
    "os.makedirs(\"gnn\", exist_ok=True)\n",
    "for ds, split, date_range in zip([train_dataset, valid_dataset, test_dataset], DATA_SPLITS, [TRAIN_DATE_RANGE, VALID_DATE_RANGE, TEST_DATE_RANGE]):\n",
    "    preds_df = generate_predictions(ds, model, date_range)\n",
    "    print(mean_squared_error(preds_df.speed_kmh, preds_df[f\"{MODEL_NAME}_speed\"]))\n",
    "    preds_df.to_parquet(f\"gnn/{split}.parquet\")\n",
    "    S3.upload_file(f\"gnn/{split}.parquet\", S3_BUCKET, f\"{S3_SUBDIR}/model_predictions/{MODEL_NAME}/{split}.parquet\")\n",
    "    print(f\"Saved {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0871b2-4fd9-42ff-a713-6693d69aa560",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Time series visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcdd109-5cfb-4e13-8639-2018c0e5b32f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_random_edge_and_neighbours_time_series(subgraph_speeds_df, dataset, model, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02bcbc63-06eb-445c-a2e8-b35096657f36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_random_edge_and_neighbours_time_series(subgraph_speeds_df, dataset, model, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902e71ed-6b92-4ace-b203-c25f54b345b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_random_edge_and_neighbours_time_series(subgraph_speeds_df, dataset, model, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69cb13c-9373-4d1f-a9ef-7bb2daf8c0e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_and_neighbours_time_series((248729659, 6258431109), subgraph_speeds_df, dataset, model, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924bd040-70ed-47ed-8f18-0ef2eef6a9d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "A3TGCN",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
