{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554cdb78-6492-4009-8ced-3850315bc7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Import libraries and define globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b68a678-4b38-4597-bffa-0d37001bbae3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyrosm tqdm folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c95e606b-56fa-403c-b7d2-f3f86f1daf9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d53c2a5-a62c-46d1-bb19-fec31052fc57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install -q torch-geometric-temporal==0.54.0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7334fe4c-cf68-4141-b7ec-bd11bc22ae1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyrosm\n",
    "from tqdm import tqdm\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal, temporal_signal_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5601a8fb-1f67-4825-9fc3-25795253374c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9568591-cd8e-4ad7-8a19-96683bab7aec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CITY_ID = 1_000_000\n",
    "MAP_FILE = f\"{CITY_ID}-latest.osm.pbf\"\n",
    "LABEL = \"speed_kmh\"\n",
    "S3 = boto3.client('s3')\n",
    "S3_BUCKET = \"some_bucket\"\n",
    "S3_SUBDIR = f\"subdir_path\"\n",
    "S3_DATA = \"data_path\"\n",
    "S3_PREDS = f\"{S3_SUBDIR}/model_preds\"\n",
    "S3_FILENAME = \"edge_time_aggregated_4_lags.parquet\"\n",
    "N_WEEKS = 7\n",
    "N_WEEKS_TRAINING = 5\n",
    "N_WEEKS_VALIDATION = 1\n",
    "TRAIN_RATIO = N_WEEKS_TRAINING / N_WEEKS\n",
    "DATA_SPLITS = [\"train\", \"valid\", \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742d19bf-8a64-4a67-8d3d-f16a1250cfe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/unique_edges.pickle\", \"unique_edges.pickle\")\n",
    "with open(\"unique_edges.pickle\", \"rb\") as f:\n",
    "    UNIQUE_EDGES = pickle.load(f)\n",
    "len(UNIQUE_EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e61fb9-0727-4171-a030-dfa63743f2ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = f\"gnn_2_gats_{EPOCHS}_hidden_channels_{HIDDEN_CHANNELS}_epochs_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks\"\n",
    "GNN_DATASET_NAME = f\"gnn_dataset_{len(UNIQUE_EDGES)}_edges_{N_WEEKS}_weeks_normalised\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99282d87-9c82-4b9a-9812-af9cf781c47d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix():\n",
    "    adjacency_matrix = np.zeros((len(UNIQUE_EDGES), len(UNIQUE_EDGES)))\n",
    "\n",
    "    for i, edge_i in enumerate(UNIQUE_EDGES):\n",
    "        for j, edge_j in enumerate(UNIQUE_EDGES):\n",
    "            if set(edge_i).intersection(set(edge_j)):\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1\n",
    "\n",
    "    adjacency_matrix = adjacency_matrix.astype(np.float32)\n",
    "    edge_index = (np.array(adjacency_matrix) > 0).nonzero()\n",
    "    return adjacency_matrix, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b08e54dd-f619-4bff-a416-2c3292c63056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data imputation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35aa2bec-ab60-412d-ba77-a93db266a933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fallback_to_past(edge, minute_bucket, fallback_horizon, unit='m'):\n",
    "    return DATASET_DICT.get((edge, minute_bucket - pd.Timedelta(fallback_horizon, unit=unit)))\n",
    "\n",
    "\n",
    "def neighbour_average(edge, minute_bucket):\n",
    "    neighbour_indicies = np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]\n",
    "    neighbour_speeds = []\n",
    "    for idx in neighbour_indicies:\n",
    "        speed = DATASET_DICT.get((edge, minute_bucket))\n",
    "        if speed is None or math.isnan(speed):\n",
    "            continue\n",
    "        neighbour_speeds.append(speed)\n",
    "    return np.mean(neighbour_speeds)\n",
    "\n",
    "\n",
    "def expand_edge_time_series(edge_df):\n",
    "    edge_df = (edge_df.reset_index().set_index(\"minute_bucket\")\n",
    "        .join(DATASET_RANGE_DF, how=\"right\", lsuffix='l')\n",
    "        .drop([\"index\", \"indexl\"], axis=1))\n",
    "    edge_df[\"edge\"] = edge_df.edge.ffill().bfill()\n",
    "    edge_df = edge_df.reset_index()\n",
    "    return edge_df\n",
    "    \n",
    "\n",
    "def compute_rolling_mean(speeds_df, window):\n",
    "    rolling_window_speed_avg_df = (\n",
    "        pd.concat([expand_edge_time_series(g) for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].groupby(\"edge\")])\n",
    "        .set_index(\"minute_bucket\").groupby(\"edge\").rolling(window).mean())\n",
    "    rolling_window_speed_avg_df.dropna(inplace=True)\n",
    "    return rolling_window_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "\n",
    "\n",
    "def impute_nan(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    for horizon, unit in [(1, 'W'), (2, 'W')]:\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = fallback_to_past(edge, minute_bucket, 15, 'm')\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "\n",
    "    if speed is None:\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_15_MIN_BUCKET_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "    else:\n",
    "        return speed\n",
    "    return speed\n",
    "\n",
    "\n",
    "def impute_dataset(speeds_df, imputation_method):\n",
    "    \"\"\"Iterate over a speeds data frame in 15-minute interval groups, fill missing values, collect into a list of snapshots.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    feature_imputation_count = 0\n",
    "    target_imputation_count = 0\n",
    "    target_mask = np.ones((len(DATASET_DATE_RANGE), len(UNIQUE_EDGES)), dtype=int)\n",
    "    for i, (minute_bucket, minute_bucket_group) in enumerate(tqdm(speeds_df.groupby(\"minute_bucket\"))):\n",
    "        edge_dict = minute_bucket_group[[\"edge\", \"speed_kmh\"] + SPEED_FEATURES].set_index(\"edge\").to_dict()\n",
    "        measurements = []\n",
    "        targets = []\n",
    "        past_hour = [(minute, minute_bucket - pd.to_timedelta(minute, unit='m')) for minute in [15, 30, 45, 60]]\n",
    "        next_15 = minute_bucket + pd.to_timedelta(15, unit='m')\n",
    "        for j, edge in enumerate(UNIQUE_EDGES):\n",
    "            row = []\n",
    "            for minute, quarter in past_hour:\n",
    "                speed = edge_dict[f\"speed_kmh_lag_{minute}_m\"].get(edge)\n",
    "                if speed is None or math.isnan(speed):\n",
    "                    speed = imputation_method(edge, quarter)\n",
    "                    feature_imputation_count += 1\n",
    "                row.append(speed)\n",
    "            measurements.append(row)\n",
    "            speed = edge_dict[\"speed_kmh\"].get(edge)\n",
    "            if speed is None or math.isnan(speed):\n",
    "                # TODO: not the most efficient way of skipping unpopular segments\n",
    "                # These are the segments that linear regression couldn't be trained on due to insufficient amount of data\n",
    "                speed = imputation_method(edge, next_15)\n",
    "                target_imputation_count += 1\n",
    "                target_mask[i, j] = 0\n",
    "            targets.append(speed)\n",
    "        xs.append(measurements)\n",
    "        ys.append(targets)\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ys = np.array(ys, dtype=np.float32)\n",
    "\n",
    "    print(f\"Feature imputation count: {feature_imputation_count}\")\n",
    "    print(f\"Target imputation count: {target_imputation_count}\")\n",
    "    print(f\"Total number of values: {len(UNIQUE_EDGES) * len(DATASET_DATE_RANGE) * 5}\")\n",
    "    print()\n",
    "\n",
    "    return xs, ys, target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e3da01-c0c2-47f0-a266-369b1a9421fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314ddec2-891c-4b70-83b7-7fa5f7728c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_global_mean_baseline(dataset):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for snapshot in dataset:\n",
    "        mse += (((MEAN_SPEED - snapshot.y)*snapshot.mask)**2).sum() / snapshot.mask.sum()\n",
    "        mae += (np.abs((MEAN_SPEED - snapshot.y)*snapshot.mask)).sum() / snapshot.mask.sum()\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def evaluate_edge_average_baseline(dataset):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for snapshot in dataset:\n",
    "        snapshot_mse = 0\n",
    "        snapshot_mae = 0\n",
    "        for i, edge in enumerate(UNIQUE_EDGES):\n",
    "            snapshot_mse += snapshot.mask[i] * (EDGE_AVG_DICT.get(edge, MEAN_SPEED) - snapshot.y[i])**2\n",
    "            snapshot_mae += snapshot.mask[i] * np.abs(EDGE_AVG_DICT.get(edge, MEAN_SPEED) - snapshot.y[i])\n",
    "        snapshot_mse /= snapshot.mask.sum()\n",
    "        snapshot_mae /= snapshot.mask.sum()\n",
    "        mse += snapshot_mse\n",
    "        mae += snapshot_mae\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "def edge_time_naive(edge, timestamp):\n",
    "    weekday = timestamp.weekday()\n",
    "    hour = timestamp.hour\n",
    "    minute = timestamp.minute\n",
    "    return EDGE_15_MIN_BUCKET_DICT.get((edge, weekday, hour, minute), EDGE_AVG_DICT.get(edge, MEAN_SPEED))\n",
    "\n",
    "\n",
    "def rolling_edge_time_avg_naive(edge, minute_bucket):\n",
    "    return ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket), MINUTE_BUCKET_AVG_DICT.get((minute_bucket - pd.Timedelta(15, unit='m')), MEAN_SPEED))\n",
    "\n",
    "\n",
    "def evaluate_edge_time_average_baseline(dataset, date_range, naive):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "    for timestamp, snapshot in zip(date_range, dataset):\n",
    "        snapshot_mse = 0\n",
    "        snapshot_mae = 0\n",
    "        for i, edge in enumerate(UNIQUE_EDGES):\n",
    "            snapshot_mse += snapshot.mask[i] * (naive(edge, timestamp) - snapshot.y[i])**2\n",
    "            snapshot_mae += snapshot.mask[i] * np.abs(naive(edge, timestamp) - snapshot.y[i])\n",
    "        snapshot_mse /= snapshot.mask.sum()\n",
    "        snapshot_mae /= snapshot.mask.sum()\n",
    "        mse += snapshot_mse\n",
    "        mae += snapshot_mae\n",
    "    mse /= dataset.snapshot_count\n",
    "    mae /= dataset.snapshot_count\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed00397-ffca-428c-87a2-c936ac3e6367",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5d915d-33d2-426b-a4bb-d147a992d16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    return (x - MEAN) / STD\n",
    "\n",
    "\n",
    "def denormalise(x):\n",
    "    return x * STD + MEAN\n",
    "    \n",
    "\n",
    "def extract_city_graph():\n",
    "    S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{CITY_ID}-latest.osm.pbf\", \"bucharest.pbf\")\n",
    "\n",
    "    osm = pyrosm.OSM(\"bucharest.pbf\")\n",
    "    nodes, edges = osm.get_network(nodes=True, network_type=\"driving+service\")\n",
    "    edges[\"edge\"] = list(zip(edges.u, edges.v))\n",
    "    print(f\"Unique OSM nodes: {nodes.id.nunique()}, unique OSM edges: {edges.id.nunique()}\")\n",
    "\n",
    "    if not os.path.isfile(S3_FILENAME):\n",
    "        S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/{S3_FILENAME}\", S3_FILENAME)\n",
    "    \n",
    "    speeds_df = pd.read_parquet(S3_FILENAME)\n",
    "\n",
    "    print(f\"Dataset time boundaries: {speeds_df.minute_bucket.min(), speeds_df.minute_bucket.max()}\")\n",
    "    print(f\"Initial dataset shape: {speeds_df.shape}\")\n",
    "\n",
    "    speeds_df[\"edge\"] = list(zip(speeds_df.start_node, speeds_df.end_node))\n",
    "\n",
    "    speeds_df = speeds_df[speeds_df.edge.isin(UNIQUE_EDGES)]\n",
    "\n",
    "    print(f\"Dataset shape after filtering edges of interest: {speeds_df.shape}\")\n",
    "\n",
    "    speeds_df[\"day\"] = speeds_df.minute_bucket.dt.weekday\n",
    "    speeds_df[\"hour\"] = speeds_df.minute_bucket.dt.hour\n",
    "    speeds_df[\"minute\"] = speeds_df.minute_bucket.dt.minute\n",
    "    speeds_df.sort_values([\"edge\", \"minute_bucket\"], inplace=True)\n",
    "\n",
    "    return speeds_df, nodes, edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5e1aa4-424a-4b2c-8123-2d0bafd66490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experimentation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9569a3-21cd-44dc-bbfd-68ef2d365e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(speeds_df, imputation_method):\n",
    "    if os.path.isfile(f\"{GNN_DATASET_NAME}.pickle\"):\n",
    "        with open(f\"{GNN_DATASET_NAME}.pickle\", \"rb\") as f:\n",
    "            dataset = pickle.load(f)\n",
    "        print(\"Loadeded imputed data\")\n",
    "    else:\n",
    "        print(\"Running data imputation ...\")\n",
    "        xs, ys, target_mask = impute_dataset(speeds_df, imputation_method)\n",
    "        dataset = StaticGraphTemporalSignal(EDGE_INDEX, ADJACENCY_MATRIX[ADJACENCY_MATRIX>0], xs, ys, mask=target_mask)\n",
    "        with open(f\"{GNN_DATASET_NAME}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        S3.upload_file(f\"{GNN_DATASET_NAME}.pickle\", S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/gnn/{GNN_DATASET_NAME}.pickle\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def evaluate_baselines(train_dataset, valid_dataset, test_dataset):\n",
    "    for naive_name, naive_method in zip([\"Global mean\", \"Edge mean\"], [evaluate_global_mean_baseline, evaluate_edge_average_baseline]):\n",
    "        for split, ds in zip([\"train\", \"valid\", \"test\"], [train_dataset, valid_dataset, test_dataset]):\n",
    "            mse, mae = naive_method(ds)\n",
    "            print(f\"\\t {naive_name} {split} MSE {mse:.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} RMSE {np.sqrt(mse):.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} MAE {mae:.{2}f}\")\n",
    "\n",
    "    for naive_name, naive_method in zip([\"Edge time naive\", \"Edge time rolling\"], [edge_time_naive, rolling_edge_time_avg_naive]):\n",
    "        for split, date_range, ds in zip([\"train\", \"valid\", \"test\"], [TRAIN_DATE_RANGE, VALID_DATE_RANGE, VALID_DATE_RANGE], [train_dataset, valid_dataset, test_dataset]):\n",
    "            mse, mae = evaluate_edge_time_average_baseline(ds, date_range, naive_method)\n",
    "            print(f\"\\t {naive_name} {split} MSE {mse:.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} RMSE {np.sqrt(mse):.{2}f}\")\n",
    "            print(f\"\\t {naive_name} {split} MAE {mae:.{2}f}\")\n",
    "\n",
    "\n",
    "def split_dataset(dataset):\n",
    "    train_dataset, valid_dataset = temporal_signal_split(dataset, train_ratio=TRAIN_RATIO)\n",
    "    valid_dataset, test_dataset = temporal_signal_split(valid_dataset, train_ratio=1/2) # Assume valid and test dataset are of equal length\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b524898-ad63-4d2b-a144-64c8f27eba80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb37e2f-3a0d-476c-8504-b04bd6a0dd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df, nodes, edges = extract_city_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4025663b-2cfd-4f4c-a635-a2991d3e29df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATASET_END_DATE = subgraph_speeds_df.minute_bucket.max()\n",
    "DATASET_START_DATE = DATASET_END_DATE - pd.Timedelta(N_WEEKS, 'W')\n",
    "TRAIN_DATE_RANGE = pd.date_range(DATASET_START_DATE, DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W'), freq=\"15min\", inclusive=\"left\")\n",
    "VALID_DATE_RANGE = pd.date_range(TRAIN_DATE_RANGE[-1], TRAIN_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")\n",
    "TEST_DATE_RANGE = pd.date_range(VALID_DATE_RANGE[-1], VALID_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")\n",
    "DATASET_DATE_RANGE = pd.concat([TRAIN_DATE_RANGE.to_series(), VALID_DATE_RANGE.to_series(), TEST_DATE_RANGE.to_series()])\n",
    "DATASET_RANGE_DF = pd.DataFrame(DATASET_DATE_RANGE, columns=[\"minute_bucket\"]).reset_index().set_index(\"minute_bucket\")\n",
    "\n",
    "SPEED_FEATURES = [col_name for col_name in subgraph_speeds_df.columns if \"lag\" in col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc192619-b948-4d7e-94f4-eb4b96786900",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATASET_START_DATE, DATASET_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827b34f2-7d43-4047-8d58-019e9ac5b22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket >= DATASET_START_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "380d998a-5cb1-4f32-bb4f-0ba9bf7cbcb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0fb1d4-ee7d-4359-a16f-f0a7ce2241c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket <= TRAIN_DATE_RANGE[-1]]\n",
    "MEAN = train_subgraph_speeds_df.speed_kmh.mean()\n",
    "STD = train_subgraph_speeds_df.speed_kmh.std()\n",
    "MEAN, STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33f822f-f725-4766-84e5-887bc00821ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EDGE_IDX_MAP = {edge: i for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "IDX_EDGE_MAP = {i: edge for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "\n",
    "MEAN_SPEED = train_subgraph_speeds_df.speed_kmh.mean()\n",
    "EDGE_AVG_DICT = train_subgraph_speeds_df[[\"speed_kmh\", \"edge\"]].groupby(\"edge\").mean().astype(int).to_dict()[\"speed_kmh\"]\n",
    "EDGE_15_MIN_BUCKET_DICT = train_subgraph_speeds_df.groupby([\"edge\", \"day\", \"hour\", \"minute\"])[\"speed_kmh\"].mean().to_dict()\n",
    "\n",
    "with open(\"edge_15min_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(EDGE_15_MIN_BUCKET_DICT, f)\n",
    "\n",
    "S3.upload_file(\"edge_15min_dict.pickle\", S3_BUCKET, f\"{S3_SUBDIR}/models/edge_15min_dict.pickle\")\n",
    "\n",
    "ADJACENCY_MATRIX, EDGE_INDEX = compute_adjacency_matrix()\n",
    "\n",
    "rolling_speed_avg_df = (pd.concat([expand_edge_time_series(g)\n",
    "    for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]]\n",
    "    .groupby(\"edge\")]).set_index(\"minute_bucket\").groupby(\"edge\").expanding().mean())\n",
    "rolling_speed_avg_df.dropna(inplace=True)\n",
    "ROLLING_EDGE_TIME_AVG_DICT = rolling_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "# TODO: Move these to data imputation methods\n",
    "ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"1h\")\n",
    "ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"2h\")\n",
    "ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"3h\")\n",
    "ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"4h\")\n",
    "\n",
    "DATASET_DICT = subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].set_index([\"edge\", \"minute_bucket\"]).to_dict()[\"speed_kmh\"]\n",
    "MINUTE_BUCKET_AVG_DICT = subgraph_speeds_df[[\"minute_bucket\", \"speed_kmh\"]].groupby(\"minute_bucket\").mean().to_dict()[\"speed_kmh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488995f9-0931-4cc9-8bf4-03cd256c5b97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/gnn/{GNN_DATASET_NAME}.pickle\", f\"{GNN_DATASET_NAME}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf931e0-6ff4-473b-8a86-cee1b0a41640",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rm \"{GNN_DATASET_NAME}.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1935bbe6-a23a-4aa8-b01b-26a76d72cf77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df.speed_kmh.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e67407c-23c0-4bcc-b310-5c3c842f524e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = prepare_dataset(subgraph_speeds_df, impute_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33be4457-9c42-4db3-8c89-b16827bf62b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00f51514-b32b-419e-a1c7-8208df4bf22b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluate_baselines(train_dataset, valid_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb541410-0e1d-4d17-a4de-31b16c8e11c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Naive baselines 317 edges 5 training weeks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
