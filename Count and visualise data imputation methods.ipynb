{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396d9293-8e6b-475b-9a69-82c492cf6374",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Installing libs and defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "127970d1-8699-40ae-b87e-621d9adcfa27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tqdm folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b66716-a226-4d82-b775-0a6a5f43c281",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb471bce-f722-4729-97e5-c62986c14eb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CITY_ID = 1_000_000\n",
    "MAP_FILE = f\"{\n",
    "    \n",
    "}-latest.osm.pbf\"\n",
    "S3 = boto3.client('s3')\n",
    "S3_BUCKET = \"some_bucket\"\n",
    "S3_SUBDIR = f\"subdir_path\"\n",
    "S3_DATA = \"data_path\"\n",
    "S3_FILENAME = \"edge_time_aggregated_4_lags.parquet\"\n",
    "DATASET_START_DATE = pd.Timestamp(2023, 7, 3, 0, 0, 0)\n",
    "N_WEEKS = 4\n",
    "N_WEEKS_TRAINING = 2\n",
    "N_WEEKS_VALIDATION = 1\n",
    "DATASET_END_DATE = DATASET_START_DATE + pd.Timedelta(N_WEEKS, 'W')\n",
    "DATASET_DATE_RANGE = pd.date_range(DATASET_START_DATE, DATASET_END_DATE, freq=\"15min\", closed=\"left\")\n",
    "DATASET_RANGE_DF = pd.DataFrame(DATASET_DATE_RANGE, columns=[\"minute_bucket\"]).reset_index().set_index(\"minute_bucket\")\n",
    "SUBGRAPH_K = 20\n",
    "CENTRAL_NODE_ID = 2389982923 # Some popular node in the city centre of Bucharest\n",
    "TRAIN_RATIO = 1/2\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664c8c5f-1790-4a10-95a5-bfe04c303563",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_adjacency_matrix():\n",
    "    adjacency_matrix = np.zeros((len(UNIQUE_EDGES), len(UNIQUE_EDGES)))\n",
    "\n",
    "    for i, edge_i in enumerate(UNIQUE_EDGES):\n",
    "        for j, edge_j in enumerate(UNIQUE_EDGES):\n",
    "            if set(edge_i).intersection(set(edge_j)):\n",
    "                adjacency_matrix[i, j] = 1\n",
    "                adjacency_matrix[j, i] = 1\n",
    "\n",
    "    adjacency_matrix = adjacency_matrix.astype(np.float32)\n",
    "    edge_index = (np.array(adjacency_matrix) > 0).nonzero()\n",
    "    return adjacency_matrix, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fedb508-da7d-4819-b1b4-d5e008ef5900",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data imputation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af9fd5c2-2bd3-476a-b33e-1d45c393f8c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fallback_to_past(edge, minute_bucket, fallback_horizon, unit='m'):\n",
    "    return DATASET_DICT.get((edge, minute_bucket - pd.Timedelta(fallback_horizon, unit=unit)))\n",
    "\n",
    "\n",
    "def neighbour_average(edge, minute_bucket):\n",
    "    neighbour_indicies = np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]\n",
    "    neighbour_speeds = []\n",
    "    for idx in neighbour_indicies:\n",
    "        speed = DATASET_DICT.get((edge, minute_bucket))\n",
    "        if speed is None or math.isnan(speed):\n",
    "            continue\n",
    "        neighbour_speeds.append(speed)\n",
    "    return np.mean(neighbour_speeds)\n",
    "\n",
    "\n",
    "def expand_edge_time_series(edge_df):\n",
    "    edge_df = (edge_df.reset_index().set_index(\"minute_bucket\")\n",
    "        .join(DATASET_RANGE_DF, how=\"right\", lsuffix='l')\n",
    "        .drop([\"index\", \"indexl\"], axis=1))\n",
    "    edge_df[\"edge\"] = edge_df.edge.ffill().bfill()\n",
    "    edge_df = edge_df.reset_index()\n",
    "    return edge_df\n",
    "    \n",
    "\n",
    "def neighbour_based_impute_nan(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge 15 minutes ago\n",
    "        2. Speed on the same edge 30 minutes ago\n",
    "        3. Speed on the same edge 45 minutes ago\n",
    "        4. Speed on the same edge 60 minutes ago\n",
    "        5. Speed on the same edge at the same time 1 week ago\n",
    "        6. Speed on the same edge at the same time 2 weeks ago \n",
    "        7. Average neighbour speed 15 minutes ago\n",
    "        8. Average over all past values before current timestamp for the current edge\n",
    "        9. Average accross all edges 15 minutes ago\n",
    "        10. Global mean speed\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for horizon, unit in FALLBACK_HORIZONS:\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed, i\n",
    "        i += 1\n",
    "        \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        i += 1\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        i += 1\n",
    "        speed = MINUTE_BUCKET_AVG_DICT.get((minute_bucket - pd.Timedelta(15, unit='m')))\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    return speed, i\n",
    "\n",
    "\n",
    "def reranked_neighbour_based_impute_nan(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    for i, (horizon, unit) in enumerate([(1, 'W'), (2, 'W')]):\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed, i\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "    else:\n",
    "        return speed, 2\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "    else:\n",
    "        return speed, 3\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MINUTE_BUCKET_AVG_DICT.get((minute_bucket - pd.Timedelta(15, unit='m')))\n",
    "    else:\n",
    "        return speed, 4\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    return speed, i\n",
    "\n",
    "\n",
    "def imputation_3(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for i, (horizon, unit) in enumerate([(1, 'W'), (2, 'W')]):\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed, i\n",
    "        i += 1\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = fallback_to_past(edge, minute_bucket, 15, 'm')\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "\n",
    "    if speed is None:\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MINUTE_BUCKET_AVG_DICT.get((minute_bucket - pd.Timedelta(15, unit='m')))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    return speed, i\n",
    "\n",
    "\n",
    "def imputation_4(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for i, (horizon, unit) in enumerate([(1, 'W'), (2, 'W')]):\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed, i\n",
    "        i += 1\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = fallback_to_past(edge, minute_bucket, 15, 'm')\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "\n",
    "    if speed is None:\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_15_MIN_BUCKET_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    return speed, i\n",
    "\n",
    "\n",
    "\n",
    "def imputation_5(edge, minute_bucket):\n",
    "    \"\"\"Data imputation method with the following steps:\n",
    "        1. Speed on the same edge at the same time 1 week ago\n",
    "        2. Speed on the same edge at the same time 2 weeks ago \n",
    "        3. Average neighbour speed at the current timestamp a week ago\n",
    "        4. Average neighbour speed at the current timestamp 2 weeks ago\n",
    "        5. Average accross all edges 15 minutes ago\n",
    "        6. Average over all past values before current timestamp for the current edge\n",
    "        7. Global mean speed\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for i, (horizon, unit) in enumerate([(1, 'W'), (2, 'W')]):\n",
    "        speed = fallback_to_past(edge, minute_bucket, horizon, unit)\n",
    "        if speed is not None:\n",
    "            return speed, i\n",
    "        i += 1\n",
    "        \n",
    "    speed = neighbour_average(edge, minute_bucket-pd.Timedelta(1, unit='W'))\n",
    "    if math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(2, unit='W'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = fallback_to_past(edge, minute_bucket, 15, 'm')\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None or math.isnan(speed):\n",
    "        speed = neighbour_average(edge, minute_bucket-pd.Timedelta(15, unit='m'))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if math.isnan(speed):\n",
    "        speed = ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "\n",
    "    if speed is None:\n",
    "        speed = ROLLING_EDGE_TIME_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_15_MIN_BUCKET_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = EDGE_AVG_DICT.get((edge, minute_bucket))\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    \n",
    "    if speed is None:\n",
    "        speed = MEAN_SPEED\n",
    "        i += 1\n",
    "    else:\n",
    "        return speed, i\n",
    "    return speed, i\n",
    "\n",
    "def impute_dataset(speeds_df, imputation_method):\n",
    "    \"\"\"Iterate over a speeds data frame in 15-minute interval groups, fill missing values, collect into a list of snapshots.\"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    feature_imputation_count = 0\n",
    "    target_imputation_count = 0\n",
    "    imputation_methods_counts = Counter()\n",
    "    for i, (minute_bucket, minute_bucket_group) in enumerate(tqdm(speeds_df.groupby(\"minute_bucket\"))):\n",
    "        edge_dict = minute_bucket_group[[\"edge\", \"speed_kmh\"] + SPEED_FEATURES].set_index(\"edge\").to_dict()\n",
    "        measurements = []\n",
    "        targets = []\n",
    "        past_hour = [(minute, minute_bucket - pd.to_timedelta(minute, unit='m')) for minute in [15, 30, 45, 60]]\n",
    "        next_15 = minute_bucket + pd.to_timedelta(15, unit='m')\n",
    "        for j, edge in enumerate(UNIQUE_EDGES):\n",
    "            row = []\n",
    "            for minute, quarter in past_hour:\n",
    "                speed = edge_dict[f\"speed_kmh_lag_{minute}_m\"].get(edge)\n",
    "                if speed is None or math.isnan(speed):\n",
    "                    speed, method_name = imputation_method(edge, quarter)\n",
    "                    imputation_methods_counts[method_name] += 1\n",
    "                    feature_imputation_count += 1\n",
    "                row.append(speed)\n",
    "            measurements.append(row)\n",
    "            speed = edge_dict[\"speed_kmh\"].get(edge)\n",
    "            if speed is None or math.isnan(speed):\n",
    "                speed, method_name = imputation_method(edge, next_15)\n",
    "                imputation_methods_counts[method_name] += 1\n",
    "                target_imputation_count += 1\n",
    "            targets.append(speed)\n",
    "        xs.append(measurements)\n",
    "        ys.append(targets)\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ys = np.array(ys, dtype=np.float32)\n",
    "    imputation_stats = (feature_imputation_count, target_imputation_count, imputation_methods_counts)\n",
    "    return xs, ys, imputation_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d95731d-3c98-4e0b-83ef-5b342ac0d2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0171b43a-c1af-47b2-8942-9deee1a9890d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_edge_and_neighbours_time_series(speeds_df, ys):\n",
    "    edge = random.choice(UNIQUE_EDGES)\n",
    "    neighbours = [IDX_EDGE_MAP[idx] for idx in np.nonzero(ADJACENCY_MATRIX[EDGE_IDX_MAP[edge]])[0]]\n",
    "    neighbours.remove(edge)\n",
    "    for e in [edge] + neighbours:\n",
    "        plot_edge_time_series(e, speeds_df, ys)\n",
    "    # return plot_edges(nodes, [edge] + neighbours)\n",
    "    \n",
    "\n",
    "def plot_one_column(col_name, edge_speeds_df, edge_imputed_speeds, mask, imputation_codes):\n",
    "\n",
    "    edge = edge_speeds_df.edge.iloc[0]\n",
    "    offset = pd.Timedelta(0 if col_name == \"speed_kmh\" else int([s for s in col_name.split('_') if s.isdecimal()][0]), unit='m')\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "        x=edge_speeds_df.minute_bucket - offset,\n",
    "        y=edge_speeds_df[col_name],\n",
    "        mode='markers',\n",
    "        name=f'Ground Truth {col_name}'\n",
    "    ))\n",
    "\n",
    "    unique_codes = set(mask)\n",
    "    if None in unique_codes:\n",
    "        unique_codes.remove(None)\n",
    "\n",
    "    for c in unique_codes:\n",
    "        minute_buckets = []\n",
    "        speeds = []\n",
    "        for j, (code, minute_bucket) in enumerate(zip(mask, DATASET_DATE_RANGE)):\n",
    "            if code == c:\n",
    "                minute_buckets.append(minute_bucket - offset)\n",
    "                speeds.append(edge_imputed_speeds[j])\n",
    "            else:\n",
    "                continue\n",
    "        fig.add_trace(go.Scatter(x=minute_buckets, y=speeds, mode='markers', name=imputation_codes[c], marker=dict(color=COLOURS[c])))\n",
    "\n",
    "\n",
    "    # Update layout with checkboxes\n",
    "    fig.update_layout(\n",
    "        title=f\"Time series for edge {edge}\",\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(\n",
    "            title=\"Time [15-minute bucket]\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Speed [km/h]\"\n",
    "        ),\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                buttons=list([\n",
    "                    dict(label=\"Ground Truth\",\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, False, False]},\n",
    "                            {\"title\": \"Trace 1\"}]),\n",
    "                ]\n",
    "                \n",
    "                + [dict(label=imputation_codes[c],\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, False, False]}]) for c in unique_codes\n",
    "                ]\n",
    "                \n",
    "                + [dict(label=\"All\",\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, True, True]},\n",
    "                            {\"title\": \"All Traces\"}])\n",
    "                ]),\n",
    "                direction=\"down\",\n",
    "                showactive=True,\n",
    "                x=0.1,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Update layout with legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_edge_features_and_targets_time_series(speeds_df, imputation_method, imputation_codes, edge=None):\n",
    "    edge = random.choice(UNIQUE_EDGES) if edge is None else edge\n",
    "    edge_df = speeds_df[speeds_df.edge == edge]\n",
    "\n",
    "    xs, ys, _, (features_mask, target_mask) = impute_edge(speeds_df, edge, imputation_method)\n",
    "\n",
    "    plot_one_column(\"speed_kmh\", edge_df, ys, target_mask, imputation_codes)\n",
    "\n",
    "    for i, col in enumerate(['speed_kmh_lag_15_m', 'speed_kmh_lag_30_m', 'speed_kmh_lag_45_m', 'speed_kmh_lag_60_m']):\n",
    "        plot_one_column(col, edge_df, xs[:, i], features_mask[:, i], imputation_codes)\n",
    "\n",
    "\n",
    "\n",
    "def plot_edge_time_series(speeds_df, imputation_method, imputation_codes, edge=None):\n",
    "\n",
    "    edge = random.choice(UNIQUE_EDGES) if edge is None else edge\n",
    "    edge_df = speeds_df[speeds_df.edge == edge]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "        x=edge_df.minute_bucket,\n",
    "        y=edge_df.speed_kmh,\n",
    "        mode='markers',\n",
    "        name='Ground Truth'\n",
    "    ))\n",
    "\n",
    "    xs, ys, imputation_stats, (_, target_mask) = impute_edge(speeds_df, edge, imputation_method)\n",
    "    unique_codes = set(target_mask)\n",
    "    if None in unique_codes:\n",
    "        unique_codes.remove(None)\n",
    "\n",
    "    for c in unique_codes:\n",
    "        minute_buckets = []\n",
    "        speeds = []\n",
    "        for j, (code, minute_bucket) in enumerate(zip(target_mask, DATASET_DATE_RANGE)):\n",
    "            if code == c:\n",
    "                minute_buckets.append(minute_bucket)\n",
    "                speeds.append(ys[j])\n",
    "            else:\n",
    "                continue\n",
    "        fig.add_trace(go.Scatter(x=minute_buckets, y=speeds, mode='markers', name=imputation_codes[c], marker=dict(color=COLOURS[c])))\n",
    "\n",
    "\n",
    "    # Update layout with checkboxes\n",
    "    fig.update_layout(\n",
    "        title=f\"Time series for edge {edge}\",\n",
    "        title_x=0.5,\n",
    "        xaxis=dict(\n",
    "            title=\"Time [15-minute bucket]\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Speed [km/h]\"\n",
    "        ),\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                buttons=list([\n",
    "                    dict(label=\"Ground Truth\",\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, False, False]},\n",
    "                            {\"title\": \"Trace 1\"}]),\n",
    "                ]\n",
    "                \n",
    "                + [dict(label=imputation_codes[c],\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, False, False]}]) for c in unique_codes\n",
    "                ]\n",
    "                \n",
    "                + [dict(label=\"All\",\n",
    "                        method=\"update\",\n",
    "                        args=[{\"visible\": [True, True, True]},\n",
    "                            {\"title\": \"All Traces\"}])\n",
    "                ]),\n",
    "                direction=\"down\",\n",
    "                showactive=True,\n",
    "                x=0.1,\n",
    "                xanchor=\"left\",\n",
    "                y=1.1,\n",
    "                yanchor=\"top\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Update layout with legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_edges(nodes, edges):\n",
    "    m = folium.Map(location=[44.435608, 26.102297], zoom_start=15)\n",
    "\n",
    "    node_ids = [n for edge in edges for n in edge]\n",
    "\n",
    "    # Add edges to the map\n",
    "    for u, v in edges:\n",
    "        x0, y0 = nodes[nodes[\"id\"] == u][[\"lat\", \"lon\"]].iloc[0]\n",
    "        x1, y1 = nodes[nodes[\"id\"] == v][[\"lat\", \"lon\"]].iloc[0]\n",
    "        folium.PolyLine(locations=[(x0, y0), (x1, y1)], color='blue', weight=5, tooltip=f\"{u, v}\").add_to(m)\n",
    "\n",
    "    # Add nodes to the map\n",
    "    for node in node_ids:\n",
    "        x, y = nodes[nodes[\"id\"] == node][[\"lat\", \"lon\"]].iloc[0]\n",
    "        folium.CircleMarker(location=(x, y), radius=5, color='red', fill=True, fill_color='red').add_to(m)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e11114-a18c-4a31-ab7c-e0e34385fb9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Calculating imputation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8263fdb0-f242-4e0b-accb-fb3ba311e1d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "S3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/subgraph_speeds_df.parquet\", \"subgraph_speeds_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da579484-a43b-45b7-a94b-c77fe0ce7400",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subgraph_speeds_df = pd.read_parquet(\"subgraph_speeds_df.parquet\")\n",
    "subgraph_speeds_df[\"edge\"] = list(zip(subgraph_speeds_df.start_node, subgraph_speeds_df.end_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c241d56c-ad0a-4691-9b09-01947db6187c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_EDGES = subgraph_speeds_df.edge.unique()\n",
    "EDGE_IDX_MAP = {edge: i for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "IDX_EDGE_MAP = {i: edge for i, edge in enumerate(UNIQUE_EDGES)}\n",
    "\n",
    "subgraph_speeds_df[\"day\"] = subgraph_speeds_df.minute_bucket.dt.weekday\n",
    "subgraph_speeds_df[\"hour\"] = subgraph_speeds_df.minute_bucket.dt.hour\n",
    "subgraph_speeds_df[\"minute\"] = subgraph_speeds_df.minute_bucket.dt.minute\n",
    "\n",
    "subgraph_speeds_df.sort_values([\"edge\", \"minute_bucket\"], inplace=True)\n",
    "\n",
    "train_subgraph_speeds_df = subgraph_speeds_df[subgraph_speeds_df.minute_bucket < DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W')]\n",
    "MEAN_SPEED = train_subgraph_speeds_df.speed_kmh.mean()\n",
    "EDGE_AVG_DICT = train_subgraph_speeds_df[[\"speed_kmh\", \"edge\"]].groupby(\"edge\").mean().astype(int).to_dict()[\"speed_kmh\"]\n",
    "EDGE_15_MIN_BUCKET_DICT = train_subgraph_speeds_df.groupby([\"edge\", \"day\", \"hour\", \"minute\"])[\"speed_kmh\"].mean().to_dict()\n",
    "SPEED_FEATURES = [col_name for col_name in subgraph_speeds_df.columns if \"speed\" in col_name]\n",
    "\n",
    "ADJACENCY_MATRIX, EDGE_INDEX = compute_adjacency_matrix()\n",
    "\n",
    "rolling_speed_avg_df = (pd.concat([expand_edge_time_series(g)\n",
    "    for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]]\n",
    "    .groupby(\"edge\")]).set_index(\"minute_bucket\").groupby(\"edge\").expanding().mean())\n",
    "rolling_speed_avg_df.dropna(inplace=True)\n",
    "ROLLING_EDGE_TIME_AVG_DICT = rolling_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "rolling_speed_avg_df.hist()\n",
    "plt.xlabel(\"Speed [km/h]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of speeds after rolling mean\")\n",
    "\n",
    "\n",
    "def compute_rolling_mean(speeds_df, window):\n",
    "    rolling_window_speed_avg_df = (\n",
    "        pd.concat([expand_edge_time_series(g) for _, g in subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].groupby(\"edge\")])\n",
    "        .set_index(\"minute_bucket\").groupby(\"edge\").rolling(window).mean())\n",
    "    rolling_window_speed_avg_df.dropna(inplace=True)\n",
    "    return rolling_window_speed_avg_df.to_dict()[\"speed_kmh\"]\n",
    "\n",
    "\n",
    "ROLLING_1H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"1h\")\n",
    "ROLLING_2H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"2h\")\n",
    "ROLLING_3H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"3h\")\n",
    "ROLLING_4H_WINDOW_EDGE_TIME_AVG_DICT = compute_rolling_mean(subgraph_speeds_df, \"4h\")\n",
    "\n",
    "DATASET_DICT = subgraph_speeds_df[[\"edge\", \"minute_bucket\", \"speed_kmh\"]].set_index([\"edge\", \"minute_bucket\"]).to_dict()[\"speed_kmh\"]\n",
    "MINUTE_BUCKET_AVG_DICT = subgraph_speeds_df[[\"minute_bucket\", \"speed_kmh\"]].groupby(\"minute_bucket\").mean().to_dict()[\"speed_kmh\"]\n",
    "FALLBACK_HORIZONS = [(15, 'm'), (30, 'm'), (45, 'm'), (60, 'm'), (1, 'W'), (2, 'W')]\n",
    "\n",
    "TRAIN_DATE_RANGE = pd.date_range(DATASET_START_DATE, DATASET_START_DATE + pd.Timedelta(N_WEEKS_TRAINING, 'W'), freq=\"15min\", inclusive=\"left\")\n",
    "VALID_DATE_RANGE = pd.date_range(TRAIN_DATE_RANGE[-1], TRAIN_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")\n",
    "TEST_DATE_RANGE = pd.date_range(VALID_DATE_RANGE[-1], VALID_DATE_RANGE[-1] + pd.Timedelta(N_WEEKS_VALIDATION, 'W'), freq=\"15min\", inclusive=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbdeb56-2006-4215-a884-8c107b1b8ab1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xs, ys, imputation_stats = impute_dataset(subgraph_speeds_df, neighbour_based_impute_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aefc169-f284-4d1b-9ea6-2c8842e29317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputation_codes_1 = {\n",
    "    0: \"15min_ago\",\n",
    "    1: \"30min_ago\",\n",
    "    2: \"45min_ago\",\n",
    "    3: \"60min_ago\",\n",
    "    4: \"week_ago\",\n",
    "    5: \"2_weeks_ago\",\n",
    "    6: \"avg_neighbour_15min_ago\",\n",
    "    7: \"avg_of_all_past\",\n",
    "    8: \"avg_of_all_edges_15min_ago\",\n",
    "    9: \"global_avg\"\n",
    "}\n",
    "\n",
    "imputation_codes_2 = {\n",
    "    0: \"week_ago\",\n",
    "    1: \"2_weeks_ago\",\n",
    "    2: \"avg_neighbour_week_ago\",\n",
    "    3: \"avg_neighbour_2_weeks_ago\",\n",
    "    4: \"avg_of_all_past\",\n",
    "    5: \"avg_of_all_edges_15min_ago\",\n",
    "    6: \"global_avg\"\n",
    "}\n",
    "\n",
    "imputation_codes_3 = {\n",
    "    0: \"week_ago\",\n",
    "    1: \"2_weeks_ago\",\n",
    "    2: \"neighbour_avg_week_ago\",\n",
    "    3: \"neighbour_avg_2_weeks_ago\",\n",
    "    4: \"15_mins_ago\",\n",
    "    5: \"neighbour_avg_15_mins_ago\",\n",
    "    6: \"past_hour_avg\",\n",
    "    7: \"avg_of_all_past\",\n",
    "    8: \"avg_of_all_edges_15min_ago\",\n",
    "    9: \"global_avg\"\n",
    "}\n",
    "\n",
    "imputation_codes_4 = {\n",
    "    0: \"week_ago\",\n",
    "    1: \"2_weeks_ago\",\n",
    "    2: \"neighbour_avg_week_ago\",\n",
    "    3: \"neighbour_avg_2_weeks_ago\",\n",
    "    4: \"15_mins_ago\",\n",
    "    5: \"neighbour_avg_15_mins_ago\",\n",
    "    6: \"past_hour_avg\",\n",
    "    7: \"avg_of_all_past\",\n",
    "    8: \"train_edge_15min_bucket_avg\",\n",
    "    9: \"train_edge_avg\",\n",
    "    10: \"global_avg\"\n",
    "}\n",
    "\n",
    "imputation_codes_5 = {\n",
    "    0: \"week_ago\",\n",
    "    1: \"2_weeks_ago\",\n",
    "    2: \"neighbour_avg_week_ago\",\n",
    "    3: \"neighbour_avg_2_weeks_ago\",\n",
    "    4: \"15_mins_ago\",\n",
    "    5: \"neighbour_avg_15_mins_ago\",\n",
    "    6: \"past_hour_avg\",\n",
    "    7: \"past_2hours_avg\",\n",
    "    8: \"past_3hours_avg\",\n",
    "    9: \"past_4hours_avg\",\n",
    "    10: \"avg_of_all_past\",\n",
    "    11: \"train_edge_15min_bucket_avg\",\n",
    "    12: \"train_edge_avg\",\n",
    "    13: \"global_avg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b869e029-89eb-48e0-8aa2-2d6384ad797b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for code, cnt in imputation_stats[-1].items():\n",
    "    print(imputation_codes_1[code], cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cc478f-5c38-4aa9-af92-f1f55ab078f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xs, ys, imputation_stats = impute_dataset(subgraph_speeds_df, reranked_neighbour_based_impute_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c44590e-998a-4e7b-967a-043930354de1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imputation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d139188-d6ad-44fb-9253-9a19cd726c99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for code, cnt in imputation_stats[-1].items():\n",
    "    print(imputation_codes_2[code], cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e169fe-92ff-46e9-9e52-bb775f1067c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualising edge imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7776005e-729e-4c6d-b42f-01ff86631f77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def impute_edge(speeds_df, edge, imputation_method):\n",
    "    \"\"\"Iterate over a speeds data frame in 15-minute interval groups, fill missing values, collect into a list of snapshots.\"\"\"\n",
    "    one_edge_df = speeds_df[speeds_df.edge == edge]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    feature_imputation_mask = np.full((len(DATASET_DATE_RANGE), len(SPEED_FEATURES)), None)\n",
    "    target_imputation_mask = np.full(len(DATASET_DATE_RANGE), None)\n",
    "    feature_imputation_count = 0\n",
    "    target_imputation_count = 0\n",
    "    imputation_methods_counts = Counter()\n",
    "    one_edge_dicts = one_edge_df[[\"minute_bucket\", \"speed_kmh\"] + SPEED_FEATURES].set_index(\"minute_bucket\").to_dict()\n",
    "    for i, minute_bucket in enumerate(DATASET_DATE_RANGE):\n",
    "        past_hour = [(minute, minute_bucket - pd.to_timedelta(minute, unit='m')) for minute in [15, 30, 45, 60]]\n",
    "        next_15 = minute_bucket + pd.to_timedelta(15, unit='m')\n",
    "        row = []\n",
    "        for j, (minute, quarter) in enumerate(past_hour):\n",
    "            speed = one_edge_dicts[f\"speed_kmh_lag_{minute}_m\"].get(minute_bucket)\n",
    "            if speed is None or math.isnan(speed):\n",
    "                speed, method_name = imputation_method(edge, quarter)\n",
    "                imputation_methods_counts[method_name] += 1\n",
    "                feature_imputation_count += 1\n",
    "                feature_imputation_mask[i, j] = method_name\n",
    "            row.append(speed)\n",
    "        xs.append(row)\n",
    "        speed = one_edge_dicts[\"speed_kmh\"].get(minute_bucket)\n",
    "        if speed is None or math.isnan(speed):\n",
    "            speed, method_name = imputation_method(edge, next_15)\n",
    "            imputation_methods_counts[method_name] += 1\n",
    "            target_imputation_count += 1\n",
    "            target_imputation_mask[i] = method_name\n",
    "        ys.append(speed)\n",
    "    xs = np.array(xs, dtype=np.float32)\n",
    "    ys = np.array(ys, dtype=np.float32)\n",
    "    imputation_stats = (feature_imputation_count, target_imputation_count, imputation_methods_counts)\n",
    "    masks = (feature_imputation_mask, target_imputation_mask)\n",
    "    return xs, ys, imputation_stats, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0694f4d6-6431-4e95-90cc-0e606a5b5673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COLOURS = [\"red\", \"green\", \"yellow\", \"blue\", \"brown\", \"black\", \"pink\", \"orange\", \"violet\", \"gray\", \"magenta\", \"cyan\", \"coral\", \"aqua\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d34eb3b-8494-4afb-8b90-46e5e1abd493",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Target imputations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d5e303-92de-43cb-8d40-958e5cea3616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (3924023215, 6258431111))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (3924023215, 6258431111))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (3924023215, 6258431111))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4, (3924023215, 6258431111))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5, (3924023215, 6258431111))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e94808d5-3d01-48b8-9b67-7f025161c174",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "random.choices(UNIQUE_EDGES, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91531011-8631-466a-95fa-6822b0afcc4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (1390254321, 245958409))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (1390254321, 245958409))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (1390254321, 245958409))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (1390254321, 245958409))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (1390254321, 245958409))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7347de2-b61f-4856-b5bc-656ba5a97d06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (248728917, 248728918))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (248728917, 248728918))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (248728917, 248728918))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (248728917, 248728918))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (248728917, 248728918))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b645357e-85dc-4ba9-a5a9-2ac5132262da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (2351320121, 2351320128))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (2351320121, 2351320128))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (2351320121, 2351320128))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (2351320121, 2351320128))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (2351320121, 2351320128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f34224-b264-4c17-8d48-211ef4fe447b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (4490165323, 4490165315))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (4490165323, 4490165315))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (4490165323, 4490165315))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (4490165323, 4490165315))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (4490165323, 4490165315))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa55b768-706d-41c3-86ff-60b8c0377987",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (1754330932, 672078070))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (1754330932, 672078070))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (1754330932, 672078070))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (1754330932, 672078070))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (1754330932, 672078070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9199930-2eca-4bb6-a8be-410d763087db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (254373201, 6169982511))\n",
    "plot_edge_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (254373201, 6169982511))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_3, imputation_codes_3,  (254373201, 6169982511))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_4, imputation_codes_4,  (254373201, 6169982511))\n",
    "plot_edge_time_series(subgraph_speeds_df, imputation_5, imputation_codes_5,  (254373201, 6169982511))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92a2775b-e459-4eee-9ea8-94290eb565dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Features and targets imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d23c974b-6a01-492f-aba1-bd312b9eef01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_features_and_targets_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (1390254321, 245958409))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc587b4-2579-4b58-8d52-51e4e3f87e15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_features_and_targets_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (248728917, 248728918))\n",
    "plot_edge_features_and_targets_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (248728917, 248728918))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48a60c2-71eb-45f7-b065-6abd8b31ab52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot_edge_features_and_targets_time_series(subgraph_speeds_df, neighbour_based_impute_nan, imputation_codes_1,  (4490165323, 4490165315))\n",
    "plot_edge_features_and_targets_time_series(subgraph_speeds_df, reranked_neighbour_based_impute_nan, imputation_codes_2,  (4490165323, 4490165315))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e16beec-180b-4a19-86a0-e5ab2825f81a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Count and visualise data imputation methods",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
