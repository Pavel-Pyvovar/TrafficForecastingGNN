{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818b7612-96fa-49dc-a137-5de26df96862",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The input data was mapmatched, so for every order we have an average speed on the traversed segments. Additionally, in this notebook we do the interpolation step and filtering start and end of the ride.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec057145-eda9-46d2-8653-de08e3f137ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading libraries and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c53bf59-f7c3-4ac2-ac9b-952369dd18ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Â We use pyrosm to get the road network graph atm\n",
    "# We use networkx for interpolation (and could be used for shortest path finding)\n",
    "%pip install pyrosm tqdm contextily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "218610fb-cdf1-4f12-b7ab-ee5afb4e9ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import contextily as ctx\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyrosm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ce4ba8-b24c-4ca6-be41-baa2243ea95d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WEEKDAYS = range(7)\n",
    "HOURS = range(24)\n",
    "MINUTES = [0, 15, 30, 45]\n",
    "FREQ = \"15min\"\n",
    "MAX_SPEED = 110\n",
    "MIN_SPEED = 5\n",
    "# Bin count determines how many edges will be processed during model training step at the same time\n",
    "# Bins are made by doing df[bin_feature] % BIN_COUNT, where bin feature is e.g. last_node\n",
    "BIN_COUNT = 20\n",
    "# How many splits of parquet inputs to consider at once. Higher = less memory & more processing time\n",
    "BATCH_COUNT = 20\n",
    "BIN_FEATURE = \"end_node\"\n",
    "CITY_ID = 1_000_000\n",
    "CITY_NAME = \"bucharest\"\n",
    "MAP_FILE = f\"{CITY_ID}-latest.osm.pbf\"\n",
    "S3_BUCKET = \"some_bucket\"\n",
    "S3_SUBDIR = f\"subdir_path\"\n",
    "S3_DATA = \"data_path\"\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "logger = logging.getLogger(\"logger_nb\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e90fcae-b4b5-46c0-a827-7ba485ebc107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{CITY_ID}-latest.osm.pbf\", \"bucharest.pbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd26f631-00b9-40ab-8b07-34c693573fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download matched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef99bbb-3b8d-4050-9c23-0c6b5f472d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Timezone are ml_timestamp is localised to timezone already\n",
    "s3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/bucharest_traffic.tar.gz\", \"bucharest_traffic.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d266dda-3f93-4f16-af6e-d05b2c8f8d6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!tar -xf bucharest_traffic.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52380f51-ecf7-43ec-99b0-ea5b2a9c754e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!mkdir bucharest\n",
    "!mv city_id\\={CITY_ID}/* bucharest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29d2759-60f8-4798-8482-5333f20306f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls bucharest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cf3759-e144-4364-b696-c0d69da93ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Preparation code\n",
    "\n",
    "In here we do the following steps:\n",
    "\n",
    "1) Interpolation step\n",
    "2) Cutting first and last 10% of points\n",
    "\n",
    "As an output, you should now have interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96673797-ebc2-44a7-b254-865fec5db90f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CityGraph:\n",
    "    \"\"\"\n",
    "    Class shared between processes for shortest path finding (interpolation step)\n",
    "    \"\"\"\n",
    "    city_nx_graph = None\n",
    "\n",
    "@cache\n",
    "def find_shortest_paths(start_end, k):\n",
    "    start, end = start_end\n",
    "    try:\n",
    "        # Get shortest path\n",
    "        shortest_path = nx.shortest_path(CityGraph.city_nx_graph, source=start, target=end, weight=None)\n",
    "        shortest_path_length = len(shortest_path)\n",
    "    except Exception:\n",
    "        # Missing pair from graph, return none\n",
    "        return None, None\n",
    "\n",
    "    cutoff_from_shortest = len(shortest_path)\n",
    "    if len(shortest_path) - 2 > k:\n",
    "        # Too many intermediary nodes, skip.\n",
    "        return None, None\n",
    "\n",
    "    if shortest_path_length > 2:\n",
    "        # If shortest path length is longer than a direct connection (i.e. there is no direct A->B, but\n",
    "        # we have to go through some intermediate node, A->X->B, then find all alternatives with the same length\n",
    "        # cutoff_from_shortest-1 (-1) is there because A->X->C is length 2 (number of hops to reach from A to C)\n",
    "        shortest_paths = list(\n",
    "            nx.all_simple_paths(\n",
    "                CityGraph.city_nx_graph,\n",
    "                source=start,\n",
    "                target=end,\n",
    "                cutoff=cutoff_from_shortest - 1,\n",
    "            )\n",
    "        )\n",
    "        return start_end, shortest_paths\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_replacement_mapping_df(shortest_proxy_mapping: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Turn shortest_proxy_mapping into a dataframe for join\n",
    "    Output is\n",
    "    before_edge, after_edge\n",
    "         (1,3),       (1,2)\n",
    "         (1,3),       (2,3)\n",
    "    \"\"\"\n",
    "    replacement_mapping = {\n",
    "        \"before_edge\": [],\n",
    "        \"after_edge\": [],\n",
    "        \"after_start_node\": [],\n",
    "        \"after_end_node\": [],\n",
    "    }\n",
    "\n",
    "    for key, replacements in shortest_proxy_mapping.items():\n",
    "        for alternative_path in replacements:\n",
    "            for ind in range(1, len(alternative_path)):\n",
    "                replacement_mapping[\"before_edge\"].append(key)\n",
    "                replacement_mapping[\"after_edge\"].append((alternative_path[ind - 1], alternative_path[ind]))\n",
    "                replacement_mapping[\"after_start_node\"].append(alternative_path[ind - 1])\n",
    "                replacement_mapping[\"after_end_node\"].append(alternative_path[ind])\n",
    "\n",
    "    repl_mapping_df = pd.DataFrame.from_dict(replacement_mapping)\n",
    "    return repl_mapping_df\n",
    "\n",
    "\n",
    "def create_shortest_proxy_mapping(start_end_pairs, k):\n",
    "    \"\"\"\n",
    "    Find all shortest paths between each two input nodes and return following mapping\n",
    "    shortest_proxy_mapping = {\n",
    "        (1, 5): [[1,2,3,5],[1,2,4,5]]\n",
    "        ...\n",
    "    }\n",
    "    where each key is start and end node (what we try to interpolate) and values are\n",
    "    list of lists of all shortest paths of minimum length.\n",
    "\n",
    "    The step considers graph as unweighted i.e. if one alternative is in meters longer than other, but\n",
    "    there is same amount of intermediate nodes, then both are still included.\n",
    "\n",
    "    k parameter sets how many intermediate nodes are allowed, e.g. if k = 1, we can get alternatives\n",
    "    (1,3)\n",
    "\n",
    "    @param start_end_pairs: For which nodes all shortest paths are found\n",
    "    @param k: number of intermediate nodes considered\n",
    "    \"\"\"\n",
    "\n",
    "    logger.debug(\"Starting shortest path finding\")\n",
    "\n",
    "    shortest_proxy_mapping_list = Parallel(n_jobs=-1, backend=\"threading\", verbose=0)(\n",
    "        delayed(find_shortest_paths)(start_end_pair, k) for start_end_pair in start_end_pairs\n",
    "    )\n",
    "\n",
    "    # Unpack results for parallel job into shortest_proxy_mapping\n",
    "    shortest_proxy_mapping = {}\n",
    "    for start_end_pair, shortest_paths in shortest_proxy_mapping_list:\n",
    "        if start_end_pair is not None:\n",
    "            start, end = start_end_pair\n",
    "            shortest_proxy_mapping[(start, end)] = shortest_paths\n",
    "\n",
    "    replacements_count = len(shortest_proxy_mapping.keys())\n",
    "    logger.debug(f\"Number of replacements found: {replacements_count}\")\n",
    "\n",
    "    return shortest_proxy_mapping\n",
    "\n",
    "\n",
    "def interpolate_missing_edges(df: pd.DataFrame, k: int = 1, retain_all: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TODO: Refactor this to simplify.\n",
    "    k: int - maximum number of intermediary nodes\n",
    "    retain_all: bool - are old edges kept as well\n",
    "    Interpolate matching data to inbetween nodes when it is missing.\n",
    "    There are many matched edges where we have the following situation:\n",
    "    We have matched two nodes, A->C that are not directly connected in a city's graph.\n",
    "    This function finds intermediary nodes, e.g. B, A->X1->X2->...->Xk->C, such that this order\n",
    "    will end up being consecutive.\n",
    "    To do this we perform a shortest path search on unweighted OSM graph and use the shortest\n",
    "    found path as a filler.\n",
    "    In cases there are multiple shortest paths, e.g. the following situation:\n",
    "    A ------ Y\n",
    "    |.       |\n",
    "    |        |\n",
    "    X ------ C\n",
    "    Then we add both pairs A-Y, Y-C and A-X, X-C, i.e. every possible shortest path.\n",
    "    Before:\n",
    "    start_node, end_node, km/h\n",
    "    1           3          25\n",
    "    After, if 1-3 are not directly connected, but connected through 2 :\n",
    "    start_node, end_node, km/h\n",
    "    1           2          25\n",
    "    2           3          25\n",
    "\n",
    "    \"\"\"\n",
    "    logger.debug(\"Starting interpolation\")\n",
    "\n",
    "    logger.debug(f\"Initial df shape: {df.shape}\")\n",
    "    # Get unique start and end node pairs. We need to only calculate once for each edge\n",
    "    unique_start_end = df[[\"start_node\", \"end_node\"]].value_counts().index\n",
    "\n",
    "    shortest_proxy_mapping = create_shortest_proxy_mapping(unique_start_end, k)\n",
    "\n",
    "    logger.debug(\"Turn replacements into mapping\")\n",
    "    replacement_mapping_df = get_replacement_mapping_df(shortest_proxy_mapping)\n",
    "    del shortest_proxy_mapping\n",
    "\n",
    "    if replacement_mapping_df.empty:\n",
    "        # If there are no replacements, then return start\n",
    "        logger.info(\"No replacements found, return initial dataframe.\")\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    # Keep only unique pairs in replacement_mapping_df, otherwise,\n",
    "    # we might have multiple speeds on one segment, e.g. situations like\n",
    "    # the following example. In that case we would have duplicate added speeds to B-C, which we would not want.\n",
    "    #     X1\n",
    "    #    /  \\\n",
    "    #   A    B - C\n",
    "    #    \\  /\n",
    "    #     X2\n",
    "    logger.debug(f\"Replacement mapping shape before unique check: {replacement_mapping_df.shape}\")\n",
    "    replacement_mapping_df = replacement_mapping_df.drop_duplicates()\n",
    "    logger.debug(f\"Replacement mapping shape after unique check: {replacement_mapping_df.shape}\")\n",
    "\n",
    "    # Get new edges with inner join.\n",
    "    new_edges_df = df.merge(replacement_mapping_df, left_on=\"edge\", right_on=\"before_edge\", how=\"inner\")\n",
    "    new_edges_df[\"edge\"] = new_edges_df[\"after_edge\"]\n",
    "    new_edges_df[\"start_node\"] = new_edges_df[\"after_start_node\"]\n",
    "    new_edges_df[\"end_node\"] = new_edges_df[\"after_end_node\"]\n",
    "    new_edges_df = new_edges_df.drop(\n",
    "        [\"before_edge\", \"after_edge\", \"after_start_node\", \"after_end_node\"], axis=1\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Do we drop rows which were replaced? If it is replaced, then it exists in shortest_proxy_mapping\n",
    "    if not retain_all:\n",
    "        logger.debug(\"Dropping old edges\")\n",
    "        before_drop = len(df)\n",
    "        df = df[~df.edge.isin(replacement_mapping_df[\"before_edge\"].unique())]\n",
    "        logger.debug(f\"Dropped {before_drop - len(df)} rows by not retaining\")\n",
    "\n",
    "    logger.debug(f\"Number of new rows generated: {len(new_edges_df)}\")\n",
    "    logger.debug(f'Number of unique edges being replaced: {len(replacement_mapping_df[\"before_edge\"].unique())}')\n",
    "\n",
    "    # Concat old row not dropped with new rows\n",
    "    output_df = pd.concat([df.reset_index(drop=True), new_edges_df], axis=0)\n",
    "    logger.debug(f\"Final dataframe shape {output_df.shape}\")\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def load_all_edges_in_graph(map_path: Path, city_id: int):\n",
    "    \"\"\"\n",
    "    Get a list of all edges that exist in osm.pbf\n",
    "    \"\"\"\n",
    "    # Load road network. Use osmium-filtered smaller map.\n",
    "    map_filepath = Path(map_path, f\"{city_id}-latest.osm.pbf\")\n",
    "    osm = pyrosm.OSM(str(map_filepath))\n",
    "    nodes, edges = osm.get_network(nodes=True, network_type=\"driving+service\")\n",
    "    logger.info(f\"Converting {map_filepath} to NetworkX graph\")\n",
    "\n",
    "    # Save graph in a Global class that is shared between local processes.\n",
    "    CityGraph.city_nx_graph = osm.to_graph(nodes, edges, graph_type=\"networkx\")\n",
    "    edges[\"edge\"] = list(zip(edges[\"u\"], edges[\"v\"]))\n",
    "    all_edges = set()\n",
    "    for edge, oneway in zip(edges[\"edge\"], edges[\"oneway\"]):\n",
    "        # if oneway == \"-1\":\n",
    "        #    # Only add reversed\n",
    "        #    all_edges.add(tuple(reversed(edge)))\n",
    "\n",
    "        all_edges.add(edge)\n",
    "        if oneway != \"yes\":\n",
    "            # And if need, then also reversed\n",
    "            all_edges.add(tuple(reversed(edge)))\n",
    "\n",
    "    return all_edges\n",
    "\n",
    "\n",
    "def remove_start_and_end_of_ride(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Throw away first 10% and last 10% of matchpoints, as they are not representative of real traffic\n",
    "    Args:\n",
    "        df: Sorted (by id, ml_timestamp) DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Removing pickup/dropoff matchpoints\")\n",
    "    logger.info(f\"DF length before cutting: {len(df)}\")\n",
    "\n",
    "    cumcount = df.groupby(\"id\")[\"ml_timestamp\"].transform(\"cumcount\")\n",
    "    totcount = df.groupby(\"id\")[\"ml_timestamp\"].transform(\"count\")\n",
    "    df[\"count_pct\"] = cumcount / totcount\n",
    "    df = df[df[\"count_pct\"].between(0.1, 0.9)]\n",
    "    del df[\"count_pct\"]\n",
    "\n",
    "    print(f\"DF length after cutting: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "    \n",
    "def batch(iterable, n=1):\n",
    "    len_iterable = len(iterable)\n",
    "    for ndx in range(0, len_iterable, n):\n",
    "        yield iterable[ndx : min(ndx + n, len_iterable)]\n",
    "\n",
    "\n",
    "def append_to_parquet_table(dataframe, filepath=None, writer=None):\n",
    "    \"\"\"Method writes/append dataframes in parquet format.\n",
    "    This method is used to write pandas DataFrame as pyarrow Table in parquet format. If the methods is invoked\n",
    "    with writer, it appends dataframe to the already written pyarrow table.\n",
    "    :param dataframe: pd.DataFrame to be written in parquet format.\n",
    "    :param filepath: target file location for parquet file.\n",
    "    :param writer: ParquetWriter object to write pyarrow tables in parquet format.\n",
    "    :return: ParquetWriter object. This can be passed in the subsequenct method calls to append DataFrame\n",
    "        in the pyarrow Table\n",
    "    \"\"\"\n",
    "    table = pa.Table.from_pandas(dataframe)\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(filepath, table.schema)\n",
    "    writer.write_table(table=table)\n",
    "    return writer\n",
    "    \n",
    "\n",
    "def batch_interpolate_and_bin(\n",
    "    matched_data_path,\n",
    "    city_map_path,\n",
    "    binned_data_path,\n",
    "    city_id,\n",
    "    bin_feature,\n",
    "    bins,\n",
    "    batch_count,\n",
    "    delete_input_data,\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads in matched data in (batch_count) different batches.\n",
    "    For each batch bin interpolated match data into (bins) bins, depending on\n",
    "    bin_feature.\n",
    "    Bin data into several bins. Takes mod of the bin_feature column for binning.\n",
    "    The higher the bins number, the less memory it takes, but more processing time.\n",
    "    batch_count: how many batches of parquet files will be processed.\n",
    "    E.g. we have values\n",
    "    11\n",
    "    29\n",
    "    32\n",
    "    We want to split to 10 bins, then we have\n",
    "    11 % 10 -> 1\n",
    "    29 % 10 -> 9\n",
    "    32 % 10 -> 2\n",
    "    @param city_id: city_id\n",
    "    @param city_map_path: where .osm.pbf file for the city is read from\n",
    "    @param matched_data_path: Where matched data is read from\n",
    "    @param bin_feature: which integer feature to bin data by. By default it is end_edge\n",
    "    @param binned_data_path: where to put binned data\n",
    "    @param bins: how many bins to bin data into. Increase this reduce memory use, increase processing time.\n",
    "    @param batch_count: how many files are read in at once. Increase this to reduce peak memory use.\n",
    "    @param delete_input_data: if matching data is deleted after processing. Saves disk space.\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    assert 1 < bins < 1000\n",
    "    data_files = glob.glob(str(matched_data_path) + \"/*.parquet\")\n",
    "    if len(data_files) == 0:\n",
    "        # Skip all, if no matched data available\n",
    "        logger.info(f\"No matched data available for city_id {city_id} from {matched_data_path}, skipping binning\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Performing interpolate and bin on {len(data_files)} data files from folder {matched_data_path}\")\n",
    "    # Read graph\n",
    "    logger.info(f\"Loading graph for city_id {city_id}\")\n",
    "    all_edges = load_all_edges_in_graph(city_map_path, city_id=city_id)\n",
    "\n",
    "    # Collect open parquet writers\n",
    "    pq_writers = {}\n",
    "    logger.info(f\"Binning data into {bins} bins\")\n",
    "\n",
    "    # If batch count is larger than count of files, then set batch count equal to files.\n",
    "    # E.g. if we have only 4 files, then only make 4 batches.\n",
    "    batch_count = batch_count if len(data_files) >= batch_count else len(data_files)\n",
    "    # Determine batch size\n",
    "    batch_size = math.ceil(len(data_files) / batch_count)\n",
    "\n",
    "    logger.info(f\"Processing {batch_count} batches with max {batch_size} files in each.\")\n",
    "\n",
    "    for i, data_batch in enumerate(batch(data_files, batch_size)):\n",
    "        logger.info(f\"Starting to process batch {i} with {len(data_batch)} files.\")\n",
    "        if len(data_batch) == 0:\n",
    "            continue\n",
    "        sub_df = pd.concat(\n",
    "            [\n",
    "                pd.read_parquet(\n",
    "                    f,\n",
    "                    columns=[\n",
    "                        \"id\",\n",
    "                        \"start_node\",\n",
    "                        \"end_node\",\n",
    "                        \"ml_timestamp\",\n",
    "                        \"speed_kmh\",\n",
    "                    ],\n",
    "                )\n",
    "                for f in data_batch\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        sub_df[\"edge\"] = list(zip(sub_df.start_node, sub_df.end_node))\n",
    " \n",
    "        # Perform interpolation step\n",
    "        sub_df = interpolate_missing_edges(df=sub_df, k=2)\n",
    "        # Keep only edges in graph\n",
    "        sub_df = sub_df[sub_df.edge.isin(all_edges)]\n",
    "        sub_df = sub_df.drop(\"edge\", axis=1)\n",
    "\n",
    "        # TODO: We could now add this back. Removing start and end of the ride.\n",
    "        # sub_df = sub_df.sort_values([\"id\", \"ml_timestamp\"])\n",
    "        sub_df = sub_df.sort_values(\"ml_timestamp\")\n",
    "        sub_df = remove_start_and_end_of_ride(sub_df)\n",
    "\n",
    "        # Bin data by bin_feature into files\n",
    "        sub_df[\"bin_group\"] = sub_df[bin_feature] % bins\n",
    "        grouped_df = sub_df.groupby(\"bin_group\")\n",
    "\n",
    "        for key, group in grouped_df:\n",
    "            if group.empty:\n",
    "                # If empty df, then ignore\n",
    "                continue\n",
    "            # If file already exists, then append without header\n",
    "            writer = pq_writers.get(key, None)\n",
    "            group.drop([\"bin_group\", \"index\"], axis=1, inplace=True)\n",
    "\n",
    "            pq_writers[key] = append_to_parquet_table(\n",
    "                group,\n",
    "                filepath=f\"{binned_data_path}/bin_{key}.parquet\",\n",
    "                writer=writer,\n",
    "            )\n",
    "\n",
    "        # Delete input file from disk\n",
    "        if delete_input_data:\n",
    "            for filename in data_batch:\n",
    "                Path(filename).unlink(missing_ok=True)\n",
    "\n",
    "    for key, writer in pq_writers.items():\n",
    "        writer.close()\n",
    "\n",
    "    # Invalidate lru cache as it wont be used for the city anymore\n",
    "    logger.info(f\"{find_shortest_paths.cache_info()}\")\n",
    "    find_shortest_paths.cache_clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3987c02a-0441-4219-be51-5c4f456edc19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd344dc2-9ff8-4370-8c36-f837793a96e2",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch_interpolate_and_bin(\n",
    "    \"bucharest/matched_data/\",\n",
    "    \"bucharest/maps\",\n",
    "    \"bucharest\",\n",
    "    CITY_ID,\n",
    "    BIN_FEATURE,\n",
    "    BIN_COUNT,\n",
    "    BATCH_COUNT,\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d1f0a6-5102-40a7-81b2-9564ad8a4813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls bucharest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c12e47-9467-4db7-a043-723c35646ab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.read_parquet(\"bucharest/bin_0.parquet\", columns=[\"ml_timestamp\"]).ml_timestamp.agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e823986-e02c-45aa-8e31-dffe7d97ecf5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save binned data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab23c51-1727-4e0e-9e23-2cb5ef869149",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(BIN_COUNT):\n",
    "    s3.upload_file(f\"{CITY_NAME}/bin_{i}.parquet\", S3_BUCKET, f\"save/path/bin_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d88aab-f900-4d0c-bb66-bb67b3e1b36e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Download binned data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22095fb-7e7d-444b-a8c2-b73710f361bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(BIN_COUNT):\n",
    "    s3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/save/path/bin_{i}.parquet\", f\"bin_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d26dae-b483-434f-ad26-13ed1c36e0d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(BIN_COUNT):\n",
    "    pd.read_parquet(f\"bin_{i}.parquet\", columns=[\"speed_kmh\"]).hist(histtype=\"step\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "738055db-7fed-4b3a-b6c8-d86724fb4e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Filtering the dataset based on geo bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02fe649d-4717-4c99-9d51-8a918007bd31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load city edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93d5a0c2-f518-4ebe-a3c7-f79b5a3e5a79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "osm = pyrosm.OSM(\"bucharest.pbf\")\n",
    "nodes, edges = osm.get_network(nodes=True, network_type=\"driving+service\")\n",
    "edges[\"edge\"] = list(zip(edges[\"u\"], edges[\"v\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1374ded-2d5c-4af4-9446-22205e99ce6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total number of nodes in the map\", nodes.shape[0])\n",
    "print(\"Total number of edges in the map\", edges.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e03462de-436f-4b47-b303-b8d824840192",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Filtering edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6afe0ae-7674-4428-8c1f-2d91d0ebe402",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"bucharest/bin_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830e9a7f-8025-41ae-bac6-51d4f575c633",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535f546c-959f-432e-8033-4daad3c42642",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6476d595-2f3d-4606-a6bc-eccaad670d15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"edge\"] = list(zip(df.start_node, df.end_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f737b913-23fc-4446-85b9-0c65d8d42f51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.edge.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab6dd27-a00e-4e12-aed1-1087c40974bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9290e9bf-d4cb-43c3-b779-6b109470ba19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "edges[\"centroid_lon\"] = [g.centroid.x for g in edges[\"geometry\"]]\n",
    "edges[\"centroid_lat\"] = [g.centroid.y for g in edges[\"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd69241-bad8-4127-a093-6cb39cf36132",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# bbox_north, bbox_south, bbox_east, bbox_west = 44.541396, 44.334247, 26.225577, 25.966674\n",
    "bbox_north, bbox_south, bbox_east, bbox_west = 44.45, 44.41, 26.14, 26.06\n",
    "\n",
    "filtered_edges = edges[\n",
    "    (edges.centroid_lat >= bbox_south) & (edges.centroid_lat <= bbox_north) &\n",
    "    (edges.centroid_lon <= bbox_east) & (edges.centroid_lon >= bbox_west)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654a0f1f-eb76-4f9b-84eb-3d3d156b487d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filtered_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c32b6d-399f-4c9e-a2e4-36ebc07d044b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ax = filtered_edges.plot()\n",
    "ctx.add_basemap(ax, crs=filtered_edges.crs)\n",
    "# plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0b7a20-64d3-41c5-8bee-4305ad89b935",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p bbox_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b43957-916a-4255-8689-9ff74c1950cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_edges(data_path, filtered_edges):\n",
    "    for prq in glob.glob(data_path):\n",
    "        df = pd.read_parquet(prq)\n",
    "        df[\"edge\"] = list(zip(df.start_node, df.end_node))\n",
    "        print(f\"Number of edges before: {df.edge.nunique()}\")\n",
    "        print(f\"Number of samples before: {df.shape[0]}\")\n",
    "        df = df[df.edge.isin(filtered_edges.edge)]\n",
    "        print(f\"Number of edges after bbox filtering: {df.edge.nunique()}\")\n",
    "        print(f\"Number of samples after bbox filtering: {df.shape[0]}\")\n",
    "        df.drop(\"edge\", axis=1, inplace=True)\n",
    "        df.to_parquet(f\"bbox_filtered/{prq.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713dfa96-28cf-4a97-8609-74369cc97785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filter_by_edges(\"bucharest/*.parquet\", filtered_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852fe7b6-9d91-4716-a414-ae4b8cdaaeec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls bbox_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfabfc3-f99d-4d0d-aa03-fa67feb944d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for prq in glob.glob(f\"bbox_filtered/*.parquet\"):\n",
    "    s3.upload_file(prq, S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/{prq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74de20f2-c99c-4867-9bba-a4ee4c4b0cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review filtering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e66d35a-7904-4307-8ace-bf774c01af45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Download sampled dataset\n",
    "Run this if you didn't do the previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f119921e-6b26-4ba3-9d1a-c0e7b5114243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mkdir bbox_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf415610-a77e-4e76-8c63-fd89f3c748c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(BIN_COUNT):\n",
    "    s3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/bbox_filtered/bbox_filtered/bin_{i}.parquet\", f\"bbox_filtered/bin_{i}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91ee6cc3-7e7a-491b-851c-24ca41adff60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(BIN_COUNT):\n",
    "    pd.read_parquet(f\"bbox_filtered/bin_{i}.parquet\", columns=[\"speed_kmh\"]).hist(histtype=\"step\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1f536a9-5c32-45c7-acab-34ca1ff83682",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregating the dataset by edge and time bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7dbc694-9dd9-4961-a518-1594da0b0510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls bbox_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c85c8c-7614-4ec3-b46b-7e4af2b18a09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer = None\n",
    "for prq in tqdm(glob.glob(f\"bbox_filtered/*.parquet\")):\n",
    "    df = pd.read_parquet(prq)\n",
    "    df[\"minute_bucket\"] = df.ml_timestamp.dt.floor(FREQ)\n",
    "    df[\"speed_kmh\"] = df.speed_kmh.apply(lambda x: max(int(x), MIN_SPEED))\n",
    "    print(f\"Number of samples before: {df.shape[0]}\")\n",
    "    df = df.groupby([\"minute_bucket\", \"start_node\", \"end_node\"])[[\"speed_kmh\"]].median().reset_index()\n",
    "    print(f\"Number of samples after aggregation: {df.shape[0]}\")\n",
    "    df.speed_kmh.hist()\n",
    "    # writer = append_to_parquet_table(df, filepath=\"edge_time_aggregated.parquet\", writer=writer)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133a3e79-3ee4-4646-88ed-473f324ba922",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!du -h bbox_filtered/\n",
    "!du -h edge_time_aggregated.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c776aa1-fefe-450d-b2ec-aebd7918722d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"edge_time_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c3fef8-0d40-40ab-b791-1e8d627f63ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6794eff-2d16-4ba9-b3dc-3c1f1daf8bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"edge\"] = list(zip(df.start_node, df.end_node))\n",
    "df.edge.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10f7c381-af3e-403f-a588-ae8e25e0f9da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.speed_kmh.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e524626-a7c5-4f3b-b60c-5cfd6a215320",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3.upload_file(\"edge_time_aggregated.parquet\", S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/bbox_filtered/edge_time_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a8ed1e-e6ec-46e9-b251-78c7dd1ee7d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Examine preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0031368d-5636-4fd0-802f-ed0f418d3891",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3.download_file(S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/bbox_filtered/edge_time_aggregated.parquet\", \"edge_time_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d5c270-2bba-48d4-b680-fc15950b6864",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"edge_time_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e885267e-c0da-4bef-862d-fc2bd9615579",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_df = df.groupby([\"start_node\", \"end_node\", \"minute_bucket\"]).count()\n",
    "duplicates = count_df[count_df.speed_kmh > 1]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e849f2-4aa2-4027-84dd-47f1ea5ff356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8c20fc-7050-4e10-b72d-aa994bfa8078",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.minute_bucket.agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7c8e76-1abf-4f5d-99d7-70a70c4e551d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.speed_kmh.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fd1aa08-47e6-43b9-8354-1200e426aa2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Extract historical speed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ad106c-5d58-4095-8203-22c195cea6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_historical_stats(df, lags, unit='m'):\n",
    "    for lag in lags:\n",
    "        df[f\"{lag}_{unit}_lag\"] = pd.to_datetime(df['minute_bucket']) - pd.to_timedelta(lag, unit=unit)\n",
    "        df = pd.merge(df, df[[\"start_node\", \"end_node\", \"speed_kmh\", \"minute_bucket\"]],\n",
    "                left_on=[\"start_node\", \"end_node\", f\"{lag}_{unit}_lag\"],\n",
    "                right_on=[\"start_node\", \"end_node\", \"minute_bucket\"], how=\"left\", suffixes=('', f\"_lag_{lag}_{unit}\"))\n",
    "        df = df.drop(f\"minute_bucket_lag_{lag}_{unit}\", axis=1)\n",
    "        df = df.drop(f\"{lag}_{unit}_lag\", axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8897ba81-a698-4e09-9474-85ebedec07a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = get_historical_stats(df, [15, 30, 45, 60], 'm') # minutes\n",
    "# full_df = get_historical_stats(full_df, [1, 2, 3], 'W') # weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b252e16a-075b-4958-8bf4-37507cfb6afc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b83db7-d3fd-4579-a349-f68b531e4fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# full_df[\"edge\"] = list(zip(full_df.start_node, full_df.end_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3f0359-efb4-4da6-9233-206bc4055923",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# full_df = full_df.groupby(\"edge\").ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d5f4cd-582a-48b7-83fb-1a6eb447f14a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# full_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee17f9f-5c4d-4bbf-97d8-103c2804f026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.speed_kmh.agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9302588-62f7-4999-a3c6-ab5889809637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"edge_time_aggregated_4_lags.parquet\"\n",
    "df.to_parquet(file_name)\n",
    "s3.upload_file(file_name, S3_BUCKET, f\"{S3_SUBDIR}/{S3_DATA}/bbox_filtered/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff2f595f-0332-4efd-9b73-eee9d28097ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16ed1e79-9759-49d9-9c3b-33c814049069",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Traffic data preprocessing with bbox filtering without train-test split",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
